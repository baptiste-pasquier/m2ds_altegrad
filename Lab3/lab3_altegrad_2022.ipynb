{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsD-LMKT7XMt"
   },
   "source": [
    "<center><h2>ALTeGraD 2022<br>Lab Session 3: NLP Frameworks</h2> 15 / 11 / 2022<br> M. Kamal Eddine, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Baptiste PASQUIER\n",
    "\n",
    "</center>\n",
    "\n",
    "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers to pretrain and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative.\n",
    "\n",
    "In the first part of this lab, you will finetune the given model on model on CLS_Books dataset using <b>Fairseq</b> by following these steps:<br>\n",
    "\n",
    " 1- <b>Tokenize the reviews</b> (Train, Valid and Test) using trained sentencepiece tokenizer provided alongside the pretrained model.[using sentencepiece library and setting the parameter <b>out_type=str</b> in the encode function].<br>\n",
    " 2- <b>Binarize the tokenized reviews and their labels</b> using the preprocess python script provided in Fairseq.<br>\n",
    " 3- <b>Fintune the pretrained $RoBERTa_{small}^{fr}$ model</b> using the train python script provided in Fairseq.<br>\n",
    " \n",
    " Finally, you will finish the first part by training a random $RoBERTa_{small}^{fr}$ model on the CLS_Books dataset and compare the results against the pretrained model while <b>visualizing the accuracies on tensorboard</b>.\n",
    "\n",
    " In the second part of this lab, you will use <b>HuggingFace's transformers</b> library to perform the finrtuning done previously with Fairseq.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H40TxVIvEWyu"
   },
   "source": [
    "# <b>Part 1: Fairseq</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwN3KCm5Ec6r"
   },
   "source": [
    "## <b>Preparing the environment and installing libraries, model and data</b>\n",
    "\n",
    "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell). In case you are using your personal computer maket sure to:\n",
    "\n",
    "1- Use Ubuntu (or any similar linux distribution) or MacOS. <b> P.S. In case you have Windows, please use Google Colab. We won't respond to any question regarding errors on Windows. </b>\n",
    "\n",
    "2- <b>Use Anaconda</b> and create new environment if you already installed Fairseq since we will be using a slightly modified version of this library.\n",
    "\n",
    "3- <b>Do not run the following three cells</b>. Instead, use their content on your personal command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cGqZ9ZB84Cd"
   },
   "outputs": [],
   "source": [
    "!mkdir altegrad.lab3 && cd altegrad.lab3 && mkdir libs\n",
    "%cd altegrad.lab3/libs\n",
    "!git clone https://github.com/hadi-abdine/fairseq\n",
    "!pip install git+https://github.com/hadi-abdine/fairseq\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install sentencepiece\n",
    "!pip install tensorboardX\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxHffsPm-EqW"
   },
   "outputs": [],
   "source": [
    "!cd .. && mkdir models\n",
    "%cd ../models\n",
    "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267604&authkey=ANaaKIDigQPyJlM\" -O \"model_fairseq.zip\"\n",
    "!unzip model_fairseq.zip\n",
    "!rm model_fairseq.zip\n",
    "!rm -rf __MACOSX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfZ_znATNdya"
   },
   "outputs": [],
   "source": [
    "!cd .. && mkdir data\n",
    "%cd ../data\n",
    "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267606&authkey=AA8Td6LoeijplD4\" -O \"cls.books.zip\"\n",
    "!unzip cls.books.zip\n",
    "!rm cls.books.zip\n",
    "!rm -rf __MACOSX/\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvpyZEexOXHm"
   },
   "source": [
    "## <b> Number of parameters of the model</b>\n",
    "\n",
    "In this section you have to compute the number of parameters of $RoBERTa_{small}^{fr}$ using PyTorch. (<b>Hint:</b> you can check the architecture of the model using model['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j7isz60LOwlV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameters :\n",
      "16516096\n",
      "Attention layer parameters :\n",
      "1575936\n",
      "1575936\n",
      "1575936\n",
      "1575936\n",
      "Total 4 Attention layers parameters :\n",
      "6303744\n",
      "Total model parameters :\n",
      "22819840\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_parameters = 0\n",
    "model = torch.load(\"models/RoBERTa_small_fr/model.pt\")\n",
    "# your code here\n",
    "\n",
    "\n",
    "def number_parameters(list_layers):\n",
    "    result = sum([model[\"model\"][layer].shape.numel() for layer in list_layers])\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Embedding parameters :\")\n",
    "n_embedding = number_parameters(\n",
    "    [\n",
    "        \"encoder.sentence_encoder.embed_tokens.weight\",\n",
    "        \"encoder.sentence_encoder.embed_positions.weight\",\n",
    "        # 'encoder.sentence_encoder.layernorm_embedding.weight',\n",
    "        # 'encoder.sentence_encoder.layernorm_embedding.bias'\n",
    "    ]\n",
    ")\n",
    "\n",
    "n_attention_layer = 0\n",
    "\n",
    "print(\"Attention layer parameters :\")\n",
    "for i in [0, 1, 2, 3]:\n",
    "    n_attention_layer += number_parameters(\n",
    "        [\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.k_proj.weight\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.k_proj.bias\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.v_proj.weight\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.v_proj.bias\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.q_proj.weight\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.q_proj.bias\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.out_proj.weight\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.self_attn.out_proj.bias\",\n",
    "            # f'encoder.sentence_encoder.layers.{i}.self_attn_layer_norm.weight',\n",
    "            # f'encoder.sentence_encoder.layers.{i}.self_attn_layer_norm.bias',\n",
    "            f\"encoder.sentence_encoder.layers.{i}.fc1.weight\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.fc1.bias\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.fc2.weight\",\n",
    "            f\"encoder.sentence_encoder.layers.{i}.fc2.bias\",\n",
    "            # f'encoder.sentence_encoder.layers.{i}.final_layer_norm.weight',\n",
    "            # f'encoder.sentence_encoder.layers.{i}.final_layer_norm.bias'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"Total 4 Attention layers parameters :\")\n",
    "print(n_attention_layer)\n",
    "\n",
    "n_parameters = n_embedding + n_attention_layer\n",
    "\n",
    "print(\"Total model parameters :\")\n",
    "print(n_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz8fnWOSI0eF"
   },
   "source": [
    "## <b>Tokenizing the reviews</b>\n",
    "\n",
    "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets. \n",
    "\n",
    "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize the three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D-hOlotmOW7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Ce ▁livre ▁est ▁tout ▁simplement ▁mag ique ▁! ▁il ▁vaut ▁le ▁de tour ▁suspens e , ▁ humour , ▁magie , ▁triste sse , ▁courage , son t ▁parfaitement ▁re group és ▁dans ▁cet ▁ ouvrage ▁... ▁Un ▁vrai ▁chef - d ' oeuvre ▁! ▁Bien ▁que ▁le ▁sort ▁de ▁Harry ▁soit ▁pré visible , ▁en ▁raison ▁de ▁sa ▁sortie ▁quasi ▁simultan ée ▁avec ▁les ▁deux ▁tome s ▁suivant s ▁; ▁ce ▁livre ▁est ▁un ▁rég al ▁que ▁se ▁soit ▁pour ▁les ▁petits ▁ou ▁les ▁grands ▁!!! ▁Johann e ▁Kat hle en ▁Row ling ▁a ▁su ▁ali er ▁plusieurs ▁histoire s ▁qui ▁para issent ▁totalement ▁différentes ▁mais ▁avec ▁une ▁fin ▁qui ▁leur ▁est ▁commun ne ▁et ▁quasi ▁intro uv able ▁jusqu ' à ▁la ▁fin ▁! ▁Elle ▁à ▁également ▁su ▁comment ▁faire ▁pour ▁att ir er ▁une ▁génération ▁de ▁non ▁lecteur s ▁avec ▁ses ▁livres ▁\" é norm es \" ▁et ▁sans ▁images ▁! ▁Entre ▁nous ▁il ▁ya ▁de ▁quoi ▁se ▁demander ▁qui ▁le ▁magic ien ▁Harry ▁ou ▁elle ▁? ▁Bravo ▁!!!\n",
      "▁J ' ai ▁lu ▁ce ▁livre ▁car ▁dans ▁ma ▁ville , ▁tout ▁le ▁monde ▁s ' en ▁sert ▁et ▁le ▁commande . ▁C ' est ▁ma ▁pharmacie nne ▁qui ▁me ▁l ' a ▁conseil lé , ▁elle ▁a ▁tellement ▁mai gri ▁que ▁je ▁lui ▁ai ▁demandé ▁ce ▁qu ' elle ▁avait ▁fait ▁et ▁au ▁lieu ▁de ▁me ▁vendre ▁du ▁per lim pin pin ▁en ▁gél ules , ▁elle ▁m ' a ▁conseil lé ▁ce ▁livre ▁à ▁5 ▁euros . ▁Bien ▁sur , ▁il ▁faut ▁faire ▁un ▁effort ▁pour ▁perdre ▁25 ▁kilo s ▁mais ▁avec ▁le ▁livre , ▁j ' avais ▁un ▁compagno n ▁de ▁route . ▁L ' auteur ▁a ▁su ▁me ▁parler ▁simplement ▁avec ▁des ▁argument s ▁très ▁fort s ▁et ▁surtout ▁j ' ai ▁senti ▁qu ' il ▁connais s ait ▁bien ▁des ▁cas ▁comme ▁le ▁mi en . ▁Il ▁y ▁a ▁dans ▁son ▁texte ▁de ▁l ' expérience , ▁de ▁la ▁simplici té ▁et ▁de ▁la ▁com passion ▁pour ▁ceux ▁qui ▁comme ▁moi ▁viva it ▁avec ▁tout ▁ce ▁poids ▁qui ▁me ▁colla it ▁au ▁corps ▁sans ▁jamais ▁vouloir ▁partir . ▁Je ▁ne ▁crois ▁pas ▁qu ' il ▁existe ▁un ▁régime ▁miracle ▁qui ▁sur pass e ▁les ▁autres ▁mais ▁je ▁crois ▁vraiment ▁qu ' il ▁y ▁a ▁des ▁personnes ▁qui ▁sa vent ▁parler ▁aux ▁autres ▁et ▁faire ▁na it re ▁des ▁déc lic s . ▁Je ▁cro ya is ▁être ▁faible ▁mais ▁ce ▁livre ▁m ' a ▁rendu ▁forte , ▁je ▁l ' ai ▁tellement ▁anno té ▁que ▁j ' en ▁suis ▁à ▁mon ▁troisième . ▁Quand ▁on ▁est ▁très ▁grosse ▁comme ▁je ▁l ' ai ▁été , ▁les ▁non - gros ▁ne ▁vous ▁compren nent ▁pas ▁ou ▁ont ▁peur ▁de ▁vous ▁fro i sser ▁en ▁vous ▁en ▁parlant , ▁alors ▁ce ▁livre ▁a ▁été ▁comme ▁un ▁compagno n - journal . ▁Je ▁suis ▁pé di cure ▁et ▁je ▁l ' ai ▁conseil lé ▁à ▁tous ▁mes ▁clients ▁gros ▁dont ▁je ▁li s ▁la ▁so uff rance ▁sur ▁les ▁pieds ▁dé form és ▁et ▁ gon f lés . ▁je ▁rend s ▁aux ▁autres ▁le ▁service ▁que ▁m ' a ▁rendu ▁ma ▁pharmacie nne . ▁Je ▁le ▁conseille ▁à ▁tous ▁ceux ▁qui ▁so uff rent ▁car ▁après ▁avoir ▁mai gri ▁c ' est ▁un ▁tel ▁bonheur ▁que ▁j ' ai ▁accepté ▁de ▁passer ▁à ▁la ▁phase ▁3 ▁de ▁ce ▁plan ▁qui ▁ impose ▁10 ▁jours ▁de ▁consolida tion ▁pour ▁chaque ▁kilo ▁perdu ▁en ▁s ' ou v rant ▁progressive ment ▁à ▁tout . ▁Maintenant , ▁je ▁suis ▁en ▁phase ▁4 , ▁c ' est ▁à ▁dire ▁que ▁je ▁mange ▁de ▁tout ▁sauf ▁le ▁jeudi ▁où ▁je ▁contrôle . ▁Je ▁remercie rai ▁jamais ▁assez ▁l ' auteur ▁de ▁ce ▁livre .\n",
      "▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁même ▁pour ▁des ▁né o phy tes ▁en ▁physique ▁des ▁matériaux , ▁comment ▁et ▁pourquoi ▁la ▁version ▁officielle ▁concernant ▁l ' ef fond re ment ▁des ▁3 ▁tour s ▁du ▁W TC ▁ne ▁tient ▁tout ▁simplement ▁pas ▁de bou t . ▁Et ▁c ' est ▁sans ▁appel ▁! ▁Il ▁constitu e ▁un ▁très ▁bon ▁argument aire ▁scientifique ▁permettant ▁de ▁soutenir ▁l ' hy po th èse ▁la ▁plus ▁vrai sem bla ble ▁pour ▁explique r ▁ce ▁qui ▁s ' est ▁réellement ▁passé ▁à ▁New ▁York , ▁ce ▁jour - là , ▁l ' hy po th èse ▁d ' une ▁dé moli tion ▁cont rô lé e ▁! ▁A ▁lire ▁absolument ▁!\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "s = spm.SentencePieceProcessor(\n",
    "    model_file=\"models/RoBERTa_small_fr/sentencepiece.bpe.model\"\n",
    ")\n",
    "\n",
    "SPLITS = [\"train\", \"test\", \"valid\"]\n",
    "SENTS = \"review\"\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(\"data/cls.books/\" + split + \".\" + SENTS, \"r\") as f:\n",
    "        reviews = f.readlines()\n",
    "        reviews = s.encode(reviews, out_type=str)\n",
    "        reviews = [\" \".join(review) for review in reviews]\n",
    "\n",
    "        # It should look something like that\n",
    "        # ▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
    "        print(reviews[0])\n",
    "    with open(\"data/cls.books/\" + split + \".spm.\" + SENTS, \"w\") as f:\n",
    "        for review in reviews:\n",
    "            f.write(review + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGNK19XuKBk0"
   },
   "source": [
    "## <b>Binarizing the finetuning dataset</b>\n",
    "\n",
    "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
    "\n",
    "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>\n",
    "\n",
    "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
    "\n",
    "Use `!python libs/fairseq/fairseq_cli/preprocess.py --help` to get details about the arguments and visit the fairseq github repository for further help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jIF1wvWoFp4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 01:57:29 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='data/cls.books/train.spm.review', validpref='data/cls.books/valid.spm.review', testpref='data/cls.books/test.spm.review', align_suffix=None, destdir='data/cls.books/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='models/RoBERTa_small_fr/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2022-11-22 01:57:29 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2022-11-22 01:57:29 | INFO | fairseq_cli.preprocess | [None] data/cls.books/train.spm.review: 1800 sents, 284877 tokens, 0.13% replaced (by <unk>)\n",
      "2022-11-22 01:57:29 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2022-11-22 01:57:30 | INFO | fairseq_cli.preprocess | [None] data/cls.books/valid.spm.review: 200 sents, 30354 tokens, 0.135% replaced (by <unk>)\n",
      "2022-11-22 01:57:30 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2022-11-22 01:57:30 | INFO | fairseq_cli.preprocess | [None] data/cls.books/test.spm.review: 2000 sents, 311660 tokens, 0.139% replaced (by <unk>)\n",
      "2022-11-22 01:57:30 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/cls.books/input0\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='data/cls.books/train.label', validpref='data/cls.books/valid.label', testpref='data/cls.books/test.label', align_suffix=None, destdir='data/cls.books/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | [None] data/cls.books/train.label: 1800 sents, 3600 tokens, 0.0% replaced (by <unk>)\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | [None] data/cls.books/valid.label: 200 sents, 400 tokens, 0.0% replaced (by <unk>)\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | [None] data/cls.books/test.label: 2000 sents, 4000 tokens, 0.0% replaced (by <unk>)\n",
      "2022-11-22 01:57:31 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/cls.books/label\n"
     ]
    }
   ],
   "source": [
    "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
    "    --only-source \\\n",
    "    --trainpref data/cls.books/train.spm.review \\\n",
    "    --validpref data/cls.books/valid.spm.review \\\n",
    "    --testpref data/cls.books/test.spm.review \\\n",
    "    --srcdict models/RoBERTa_small_fr/dict.txt \\\n",
    "    --destdir data/cls.books/input0 \\\n",
    "    --workers 8)#fill me - binarize the tokenized reviews\n",
    "\n",
    "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
    "    --only-source \\\n",
    "    --trainpref data/cls.books/train.label \\\n",
    "    --validpref data/cls.books/valid.label \\\n",
    "    --testpref data/cls.books/test.label \\\n",
    "    --destdir data/cls.books/label \\\n",
    "    --workers 8)#fill me - binarize the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SSjBcQJnuSL"
   },
   "source": [
    "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
    "\n",
    "In this section you will use <b>fairseq/fairseq_cli/train.py</b> python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2. \n",
    "\n",
    "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JV2112YPJEDA"
   },
   "outputs": [],
   "source": [
    "DATA_SET = \"books\"\n",
    "TASK = \"sentence_prediction\"  # fill me, sentence prediction task on fairseq\n",
    "MODEL = \"RoBERTa_small_fr\"\n",
    "DATA_PATH = \"data/cls.books\"  # fill me\n",
    "MODEL_PATH = \"models/RoBERTa_small_fr/model.pt\"  # fill me\n",
    "MAX_EPOCH = 5  # fill me\n",
    "MAX_SENTENCES = 8  # fill me, batch size\n",
    "MAX_UPDATE = int(\n",
    "    MAX_EPOCH * 1800 / MAX_SENTENCES\n",
    ")  # fill me, n_epochs * n_train_examples / total batch size\n",
    "LR = 1e-5  # fill me\n",
    "VALID_SUBSET = \"valid,test\"  # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
    "METRIC = \"accuracy\"  # fill me, use the accuracy metric\n",
    "NUM_CLASSES = 2  # fill me, number of classes\n",
    "SEEDS = 3\n",
    "CUDA_VISIBLE_DEVICES = 0\n",
    "WARMUP = 6  # fill me, warmup ratio=6% of the whole training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6Mdznms-EYyz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 01:57:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', 'restore_file': 'models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', restore_file='models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls.books', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls.books', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 6, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-22 01:57:34 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2022-11-22 01:57:34 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-11-22 01:57:35 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-11-22 01:57:35 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2022-11-22 01:57:35 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2022-11-22 01:57:35 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2022-11-22 01:57:35 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2022-11-22 01:57:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-22 01:57:35 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/input0/valid\n",
      "2022-11-22 01:57:35 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/label/valid\n",
      "2022-11-22 01:57:35 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2022-11-22 01:57:35 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/input0/test\n",
      "2022-11-22 01:57:35 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/label/test\n",
      "2022-11-22 01:57:35 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2022-11-22 01:57:36 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2022-11-22 01:57:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:57:36 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 10.000 GB ; name = NVIDIA GeForce RTX 3080                 \n",
      "2022-11-22 01:57:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:57:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-22 01:57:36 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2022-11-22 01:57:36 | INFO | fairseq.trainer | Preparing to load checkpoint models/RoBERTa_small_fr/model.pt\n",
      "2022-11-22 01:57:36 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
      "2022-11-22 01:57:36 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
      "2022-11-22 01:57:36 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
      "2022-11-22 01:57:36 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
      "2022-11-22 01:57:36 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-22 01:57:36 | INFO | fairseq.trainer | Loaded checkpoint models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
      "2022-11-22 01:57:36 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-22 01:57:36 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/input0/train\n",
      "2022-11-22 01:57:36 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/label/train\n",
      "2022-11-22 01:57:36 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2022-11-22 01:57:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:57:36 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-22 01:57:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001:  99%|▉| 222/225 [00:07<00:00, 40.35it/s, loss=0.729, nll_loss=0.005, 2022-11-22 01:57:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  36%|██▌    | 9/25 [00:00<00:00, 85.67it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:57:44 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.892 | nll_loss 0.007 | accuracy 69 | wps 143228 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2022-11-22 01:57:44 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 67.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 106.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 118.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  19%|▉    | 47/250 [00:00<00:01, 112.18it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  24%|█▏   | 59/250 [00:00<00:01, 106.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  29%|█▍   | 72/250 [00:00<00:01, 111.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  34%|█▋   | 84/250 [00:00<00:01, 113.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  39%|█▉   | 97/250 [00:00<00:01, 116.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  44%|█▋  | 109/250 [00:00<00:01, 116.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  49%|█▉  | 123/250 [00:01<00:01, 122.57it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  55%|██▏ | 137/250 [00:01<00:00, 124.92it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  60%|██▍ | 151/250 [00:01<00:00, 128.92it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  66%|██▋ | 166/250 [00:01<00:00, 133.17it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  72%|██▉ | 180/250 [00:01<00:00, 128.62it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  78%|███ | 195/250 [00:01<00:00, 132.43it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  84%|███▎| 209/250 [00:01<00:00, 134.29it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  90%|███▌| 224/250 [00:01<00:00, 138.13it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  96%|███▊| 239/250 [00:01<00:00, 140.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:57:46 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.83 | nll_loss 0.006 | accuracy 70.8 | wps 133405 | wpb 1045.3 | bsz 8 | num_updates 225\n",
      "2022-11-22 01:57:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-22 01:57:46 | INFO | train | epoch 001 | loss 0.946 | nll_loss 0.007 | accuracy 62.4 | wps 28817.3 | ups 26.99 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.0429e-06 | gnorm 3.702 | train_wall 7 | gb_free 9.4 | wall 10\n",
      "2022-11-22 01:57:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 002:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:57:46 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-22 01:57:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 224/225 [00:06<00:00, 41.10it/s, loss=0.826, nll_loss=0.006, 2022-11-22 01:57:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 76.34it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 116.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:57:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.809 | nll_loss 0.006 | accuracy 70 | wps 133993 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 70\n",
      "2022-11-22 01:57:52 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 002 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   4%|▎      | 9/250 [00:00<00:02, 83.73it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  10%|▍    | 24/250 [00:00<00:01, 117.74it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  16%|▊    | 39/250 [00:00<00:01, 129.75it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  22%|█    | 54/250 [00:00<00:01, 134.61it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  28%|█▍   | 69/250 [00:00<00:01, 137.87it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  34%|█▋   | 84/250 [00:00<00:01, 140.25it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  40%|█▉   | 99/250 [00:00<00:01, 137.55it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  45%|█▊  | 113/250 [00:00<00:00, 138.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  51%|██  | 128/250 [00:00<00:00, 139.15it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  57%|██▎ | 142/250 [00:01<00:00, 136.44it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  62%|██▍ | 156/250 [00:01<00:00, 137.02it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  68%|██▋ | 171/250 [00:01<00:00, 139.17it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  74%|██▉ | 186/250 [00:01<00:00, 139.93it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  80%|███▏| 201/250 [00:01<00:00, 141.65it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  86%|███▍| 216/250 [00:01<00:00, 141.57it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  92%|███▋| 231/250 [00:01<00:00, 137.06it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  99%|███▉| 247/250 [00:01<00:00, 141.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:57:54 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.76 | nll_loss 0.006 | accuracy 74.2 | wps 145589 | wpb 1045.3 | bsz 8 | num_updates 450\n",
      "2022-11-22 01:57:54 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-22 01:57:54 | INFO | train | epoch 002 | loss 0.733 | nll_loss 0.005 | accuracy 75.8 | wps 29543.1 | ups 27.68 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.03217e-06 | gnorm 6.769 | train_wall 6 | gb_free 9.4 | wall 18\n",
      "2022-11-22 01:57:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 003:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:57:54 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-22 01:57:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  98%|▉| 221/225 [00:05<00:00, 38.69it/s, loss=0.704, nll_loss=0.005, 2022-11-22 01:58:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 66.82it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 119.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:00 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.61 | nll_loss 0.005 | accuracy 80 | wps 136842 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 80\n",
      "2022-11-22 01:58:00 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 003 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 68.85it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 107.85it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 121.26it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 127.07it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  25%|█▏   | 62/250 [00:00<00:01, 127.85it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  30%|█▌   | 75/250 [00:00<00:01, 128.29it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  36%|█▊   | 89/250 [00:00<00:01, 131.44it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  41%|█▋  | 103/250 [00:00<00:01, 132.99it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  47%|█▊  | 117/250 [00:00<00:01, 131.94it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  52%|██  | 131/250 [00:01<00:00, 132.39it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  58%|██▎ | 146/250 [00:01<00:00, 136.65it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  64%|██▌ | 161/250 [00:01<00:00, 139.34it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  70%|██▊ | 175/250 [00:01<00:00, 138.72it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  76%|███ | 189/250 [00:01<00:00, 136.46it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  81%|███▏| 203/250 [00:01<00:00, 135.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  87%|███▍| 217/250 [00:01<00:00, 136.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  92%|███▋| 231/250 [00:01<00:00, 132.39it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  99%|███▉| 247/250 [00:01<00:00, 138.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:02 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.65 | nll_loss 0.005 | accuracy 79.5 | wps 140529 | wpb 1045.3 | bsz 8 | num_updates 675\n",
      "2022-11-22 01:58:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-22 01:58:02 | INFO | train | epoch 003 | loss 0.638 | nll_loss 0.005 | accuracy 79.7 | wps 29742.8 | ups 27.87 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.02145e-06 | gnorm 7.946 | train_wall 6 | gb_free 9.4 | wall 26\n",
      "2022-11-22 01:58:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 004:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:02 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-22 01:58:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  98%|▉| 221/225 [00:05<00:00, 39.43it/s, loss=0.618, nll_loss=0.004, 2022-11-22 01:58:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 69.79it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 116.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:08 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.648 | nll_loss 0.005 | accuracy 77.5 | wps 134454 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 80\n",
      "2022-11-22 01:58:08 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 004 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 79.69it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   8%|▍    | 20/250 [00:00<00:02, 100.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  13%|▋    | 33/250 [00:00<00:01, 111.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  18%|▉    | 46/250 [00:00<00:01, 117.80it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  24%|█▏   | 60/250 [00:00<00:01, 125.20it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  30%|█▍   | 74/250 [00:00<00:01, 128.07it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  35%|█▋   | 87/250 [00:00<00:01, 126.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  40%|█▌  | 101/250 [00:00<00:01, 128.36it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  46%|█▊  | 115/250 [00:00<00:01, 131.65it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  52%|██  | 130/250 [00:01<00:00, 134.62it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  58%|██▎ | 144/250 [00:01<00:00, 135.93it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  64%|██▌ | 159/250 [00:01<00:00, 137.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  69%|██▊ | 173/250 [00:01<00:00, 129.89it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  75%|██▉ | 187/250 [00:01<00:00, 130.27it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  81%|███▏| 202/250 [00:01<00:00, 133.57it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  86%|███▍| 216/250 [00:01<00:00, 131.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  92%|███▋| 230/250 [00:01<00:00, 122.99it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  98%|███▉| 245/250 [00:01<00:00, 129.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:10 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.664 | nll_loss 0.005 | accuracy 79.2 | wps 135604 | wpb 1045.3 | bsz 8 | num_updates 900\n",
      "2022-11-22 01:58:10 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-22 01:58:10 | INFO | train | epoch 004 | loss 0.581 | nll_loss 0.004 | accuracy 82 | wps 29094.7 | ups 27.26 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.01072e-06 | gnorm 8.524 | train_wall 6 | gb_free 9.4 | wall 34\n",
      "2022-11-22 01:58:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 005:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:10 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-22 01:58:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  98%|▉| 221/225 [00:05<00:00, 38.48it/s, loss=0.382, nll_loss=0.003, 2022-11-22 01:58:16 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
      "2022-11-22 01:58:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 77.85it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|█████| 25/25 [00:00<00:00, 128.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:17 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.591 | nll_loss 0.005 | accuracy 81 | wps 142725 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 81\n",
      "2022-11-22 01:58:17 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 005 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 66.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 108.05it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 118.98it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  19%|▉    | 47/250 [00:00<00:01, 112.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  25%|█▏   | 62/250 [00:00<00:01, 122.99it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  30%|█▌   | 76/250 [00:00<00:01, 126.65it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  36%|█▊   | 91/250 [00:00<00:01, 131.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  42%|█▋  | 105/250 [00:00<00:01, 130.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  48%|█▉  | 119/250 [00:00<00:01, 121.22it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  53%|██  | 132/250 [00:01<00:00, 119.68it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  58%|██▎ | 145/250 [00:01<00:00, 112.98it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  64%|██▌ | 159/250 [00:01<00:00, 118.18it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  69%|██▊ | 172/250 [00:01<00:00, 120.71it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  74%|██▉ | 186/250 [00:01<00:00, 124.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  80%|███▏| 200/250 [00:01<00:00, 128.06it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  86%|███▍| 215/250 [00:01<00:00, 131.84it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  92%|███▋| 229/250 [00:01<00:00, 131.58it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  98%|███▉| 244/250 [00:01<00:00, 135.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:19 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.643 | nll_loss 0.005 | accuracy 79.5 | wps 132624 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
      "2022-11-22 01:58:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-22 01:58:19 | INFO | train | epoch 005 | loss 0.55 | nll_loss 0.004 | accuracy 83 | wps 28957.9 | ups 27.14 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 8.759 | train_wall 6 | gb_free 9.5 | wall 42\n",
      "2022-11-22 01:58:19 | INFO | fairseq_cli.train | done training in 42.2 seconds\n",
      "2022-11-22 01:58:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', 'restore_file': 'models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', restore_file='models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls.books', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls.books', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 6, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-22 01:58:21 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2022-11-22 01:58:21 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-11-22 01:58:22 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-11-22 01:58:22 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2022-11-22 01:58:22 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2022-11-22 01:58:22 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2022-11-22 01:58:22 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2022-11-22 01:58:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-22 01:58:22 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/input0/valid\n",
      "2022-11-22 01:58:22 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/label/valid\n",
      "2022-11-22 01:58:22 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2022-11-22 01:58:22 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/input0/test\n",
      "2022-11-22 01:58:22 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/label/test\n",
      "2022-11-22 01:58:22 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2022-11-22 01:58:23 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2022-11-22 01:58:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:58:23 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 10.000 GB ; name = NVIDIA GeForce RTX 3080                 \n",
      "2022-11-22 01:58:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:58:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-22 01:58:23 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2022-11-22 01:58:23 | INFO | fairseq.trainer | Preparing to load checkpoint models/RoBERTa_small_fr/model.pt\n",
      "2022-11-22 01:58:23 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
      "2022-11-22 01:58:23 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
      "2022-11-22 01:58:23 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
      "2022-11-22 01:58:23 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
      "2022-11-22 01:58:23 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-22 01:58:23 | INFO | fairseq.trainer | Loaded checkpoint models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
      "2022-11-22 01:58:23 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-22 01:58:23 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/input0/train\n",
      "2022-11-22 01:58:23 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/label/train\n",
      "2022-11-22 01:58:23 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2022-11-22 01:58:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:23 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-22 01:58:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001:  99%|▉| 223/225 [00:07<00:00, 38.21it/s, loss=0.757, nll_loss=0.006, 2022-11-22 01:58:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 69.47it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 114.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.852 | nll_loss 0.007 | accuracy 66 | wps 132518 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2022-11-22 01:58:31 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   2%|▏      | 5/250 [00:00<00:05, 47.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   7%|▍     | 18/250 [00:00<00:02, 93.62it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  13%|▋    | 32/250 [00:00<00:01, 111.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  18%|▉    | 44/250 [00:00<00:01, 106.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  22%|█▎    | 55/250 [00:00<00:02, 96.53it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  26%|█▌    | 65/250 [00:00<00:01, 95.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  30%|█▊    | 75/250 [00:00<00:01, 92.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  34%|██    | 85/250 [00:00<00:01, 92.48it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  38%|██▎   | 95/250 [00:01<00:01, 92.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  43%|██▏  | 107/250 [00:01<00:01, 99.51it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  48%|█▉  | 119/250 [00:01<00:01, 105.04it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  53%|██  | 132/250 [00:01<00:01, 111.75it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  58%|██▎ | 145/250 [00:01<00:00, 115.19it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  64%|██▌ | 159/250 [00:01<00:00, 122.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  69%|██▊ | 173/250 [00:01<00:00, 126.90it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  74%|██▉ | 186/250 [00:01<00:00, 120.06it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  80%|███▏| 199/250 [00:01<00:00, 118.10it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  85%|███▍| 212/250 [00:01<00:00, 120.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  90%|███▌| 225/250 [00:02<00:00, 120.09it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  95%|███▊| 238/250 [00:02<00:00, 120.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:33 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.797 | nll_loss 0.006 | accuracy 72.8 | wps 117943 | wpb 1045.3 | bsz 8 | num_updates 225\n",
      "2022-11-22 01:58:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-22 01:58:33 | INFO | train | epoch 001 | loss 0.947 | nll_loss 0.007 | accuracy 62.1 | wps 27857.5 | ups 26.14 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.0429e-06 | gnorm 3.694 | train_wall 7 | gb_free 9.4 | wall 10\n",
      "2022-11-22 01:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 002:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:33 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-22 01:58:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  99%|▉| 222/225 [00:06<00:00, 40.74it/s, loss=0.693, nll_loss=0.005, 2022-11-22 01:58:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 66.10it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  96%|████▊| 24/25 [00:00<00:00, 122.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.758 | nll_loss 0.006 | accuracy 69 | wps 139186 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 69\n",
      "2022-11-22 01:58:39 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 002 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 68.35it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   8%|▍    | 20/250 [00:00<00:02, 102.25it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 120.42it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  19%|▉    | 48/250 [00:00<00:01, 120.05it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  25%|█▎   | 63/250 [00:00<00:01, 128.59it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  31%|█▌   | 78/250 [00:00<00:01, 135.10it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  37%|█▊   | 93/250 [00:00<00:01, 137.87it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  43%|█▋  | 107/250 [00:00<00:01, 133.23it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  48%|█▉  | 121/250 [00:00<00:00, 132.20it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  54%|██▏ | 136/250 [00:01<00:00, 136.08it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  60%|██▍ | 150/250 [00:01<00:00, 135.34it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  66%|██▌ | 164/250 [00:01<00:00, 131.52it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  71%|██▊ | 178/250 [00:01<00:00, 132.65it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  77%|███ | 192/250 [00:01<00:00, 133.35it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  82%|███▎| 206/250 [00:01<00:00, 134.49it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  88%|███▌| 220/250 [00:01<00:00, 135.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  94%|███▊| 235/250 [00:01<00:00, 136.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:41 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.72 | nll_loss 0.006 | accuracy 76.3 | wps 140199 | wpb 1045.3 | bsz 8 | num_updates 450\n",
      "2022-11-22 01:58:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-22 01:58:41 | INFO | train | epoch 002 | loss 0.742 | nll_loss 0.006 | accuracy 75.7 | wps 29133.8 | ups 27.3 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.03217e-06 | gnorm 6.57 | train_wall 6 | gb_free 9.4 | wall 18\n",
      "2022-11-22 01:58:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 003:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:41 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-22 01:58:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  98%|▉| 220/225 [00:05<00:00, 40.12it/s, loss=0.551, nll_loss=0.004, 2022-11-22 01:58:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 76.79it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 117.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.639 | nll_loss 0.005 | accuracy 79 | wps 136790 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 79\n",
      "2022-11-22 01:58:47 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 003 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 69.83it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 109.24it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 122.00it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 128.27it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  26%|█▎   | 64/250 [00:00<00:01, 133.45it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  31%|█▌   | 78/250 [00:00<00:01, 125.85it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  37%|█▊   | 93/250 [00:00<00:01, 131.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  43%|█▋  | 107/250 [00:00<00:01, 133.20it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  48%|█▉  | 121/250 [00:00<00:00, 131.98it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  54%|██▏ | 135/250 [00:01<00:00, 133.61it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  60%|██▍ | 150/250 [00:01<00:00, 136.11it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  66%|██▋ | 165/250 [00:01<00:00, 137.42it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  72%|██▊ | 179/250 [00:01<00:00, 138.11it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  77%|███ | 193/250 [00:01<00:00, 138.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  83%|███▎| 207/250 [00:01<00:00, 135.23it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  88%|███▌| 221/250 [00:01<00:00, 129.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  94%|███▊| 235/250 [00:01<00:00, 130.70it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset: 100%|████| 250/250 [00:01<00:00, 136.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:49 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.654 | nll_loss 0.005 | accuracy 78.4 | wps 139624 | wpb 1045.3 | bsz 8 | num_updates 675\n",
      "2022-11-22 01:58:49 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-22 01:58:49 | INFO | train | epoch 003 | loss 0.638 | nll_loss 0.005 | accuracy 78.9 | wps 30297.3 | ups 28.39 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.02145e-06 | gnorm 7.555 | train_wall 5 | gb_free 9.5 | wall 26\n",
      "2022-11-22 01:58:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 004:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:49 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-22 01:58:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  98%|▉| 220/225 [00:05<00:00, 41.55it/s, loss=0.63, nll_loss=0.005, a2022-11-22 01:58:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 68.69it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 113.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.644 | nll_loss 0.005 | accuracy 75.5 | wps 130532 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 79\n",
      "2022-11-22 01:58:55 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 004 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   4%|▎      | 9/250 [00:00<00:02, 85.48it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   9%|▍    | 23/250 [00:00<00:01, 116.06it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  15%|▋    | 37/250 [00:00<00:01, 124.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  20%|█    | 51/250 [00:00<00:01, 128.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  26%|█▎   | 65/250 [00:00<00:01, 132.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  32%|█▌   | 79/250 [00:00<00:01, 132.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  37%|█▊   | 93/250 [00:00<00:01, 130.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  43%|█▋  | 107/250 [00:00<00:01, 132.29it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  49%|█▉  | 122/250 [00:00<00:00, 134.78it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  54%|██▏ | 136/250 [00:01<00:00, 132.21it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  60%|██▍ | 151/250 [00:01<00:00, 135.85it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  66%|██▋ | 166/250 [00:01<00:00, 138.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  72%|██▉ | 180/250 [00:01<00:00, 138.33it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  78%|███ | 195/250 [00:01<00:00, 140.56it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  84%|███▎| 210/250 [00:01<00:00, 138.81it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  90%|███▌| 224/250 [00:01<00:00, 137.75it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  95%|███▊| 238/250 [00:01<00:00, 135.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:58:57 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.659 | nll_loss 0.005 | accuracy 78.7 | wps 141866 | wpb 1045.3 | bsz 8 | num_updates 900\n",
      "2022-11-22 01:58:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-22 01:58:57 | INFO | train | epoch 004 | loss 0.588 | nll_loss 0.004 | accuracy 82.3 | wps 30152.2 | ups 28.25 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.01072e-06 | gnorm 8.305 | train_wall 5 | gb_free 9.4 | wall 34\n",
      "2022-11-22 01:58:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 005:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:58:57 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-22 01:58:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  98%|▉| 220/225 [00:05<00:00, 40.40it/s, loss=0.48, nll_loss=0.004, a2022-11-22 01:59:03 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
      "2022-11-22 01:59:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 78.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 116.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:03 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.614 | nll_loss 0.005 | accuracy 77.5 | wps 138839 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 79\n",
      "2022-11-22 01:59:03 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 005 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   4%|▎      | 9/250 [00:00<00:02, 85.06it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   9%|▍    | 23/250 [00:00<00:01, 115.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  15%|▊    | 38/250 [00:00<00:01, 129.18it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  21%|█    | 53/250 [00:00<00:01, 135.47it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  27%|█▎   | 67/250 [00:00<00:01, 134.42it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  32%|█▌   | 81/250 [00:00<00:01, 132.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  39%|█▉   | 97/250 [00:00<00:01, 138.66it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  44%|█▊  | 111/250 [00:00<00:01, 138.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  50%|██  | 125/250 [00:00<00:00, 135.89it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  56%|██▏ | 139/250 [00:01<00:00, 136.25it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  61%|██▍ | 153/250 [00:01<00:00, 134.78it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  68%|██▋ | 169/250 [00:01<00:00, 139.84it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  74%|██▉ | 184/250 [00:01<00:00, 141.63it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  80%|███▏| 199/250 [00:01<00:00, 139.53it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  86%|███▍| 214/250 [00:01<00:00, 140.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  92%|███▋| 229/250 [00:01<00:00, 135.70it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  98%|███▉| 245/250 [00:01<00:00, 140.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:05 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.648 | nll_loss 0.005 | accuracy 79.3 | wps 145040 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
      "2022-11-22 01:59:05 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-22 01:59:05 | INFO | train | epoch 005 | loss 0.553 | nll_loss 0.004 | accuracy 82.8 | wps 30171.8 | ups 28.27 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 8.476 | train_wall 6 | gb_free 9.4 | wall 42\n",
      "2022-11-22 01:59:05 | INFO | fairseq_cli.train | done training in 41.7 seconds\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', 'restore_file': 'models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', restore_file='models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls.books', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls.books', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 6, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-22 01:59:08 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2022-11-22 01:59:08 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2022-11-22 01:59:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-22 01:59:08 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/input0/valid\n",
      "2022-11-22 01:59:08 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/label/valid\n",
      "2022-11-22 01:59:08 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2022-11-22 01:59:08 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/input0/test\n",
      "2022-11-22 01:59:08 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/label/test\n",
      "2022-11-22 01:59:08 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2022-11-22 01:59:09 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2022-11-22 01:59:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:59:09 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 10.000 GB ; name = NVIDIA GeForce RTX 3080                 \n",
      "2022-11-22 01:59:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:59:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-22 01:59:09 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2022-11-22 01:59:09 | INFO | fairseq.trainer | Preparing to load checkpoint models/RoBERTa_small_fr/model.pt\n",
      "2022-11-22 01:59:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
      "2022-11-22 01:59:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
      "2022-11-22 01:59:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
      "2022-11-22 01:59:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
      "2022-11-22 01:59:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-22 01:59:09 | INFO | fairseq.trainer | Loaded checkpoint models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
      "2022-11-22 01:59:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-22 01:59:09 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/input0/train\n",
      "2022-11-22 01:59:09 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/label/train\n",
      "2022-11-22 01:59:09 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2022-11-22 01:59:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:59:09 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-22 01:59:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001:  99%|▉| 222/225 [00:06<00:00, 38.16it/s, loss=0.594, nll_loss=0.004, 2022-11-22 01:59:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 62.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 109.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.796 | nll_loss 0.006 | accuracy 71.5 | wps 127281 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2022-11-22 01:59:17 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 64.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 103.21it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 118.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 124.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  25%|█▏   | 62/250 [00:00<00:01, 110.51it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  30%|█▌   | 75/250 [00:00<00:01, 115.00it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  36%|█▊   | 89/250 [00:00<00:01, 119.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  41%|█▋  | 102/250 [00:00<00:01, 118.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  46%|█▊  | 115/250 [00:00<00:01, 121.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  52%|██  | 129/250 [00:01<00:00, 126.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  57%|██▎ | 142/250 [00:01<00:00, 127.48it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  62%|██▍ | 156/250 [00:01<00:00, 129.12it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  68%|██▋ | 170/250 [00:01<00:00, 130.33it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  74%|██▉ | 185/250 [00:01<00:00, 133.61it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  80%|███▏| 199/250 [00:01<00:00, 133.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  85%|███▍| 213/250 [00:01<00:00, 130.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  91%|███▋| 228/250 [00:01<00:00, 133.69it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  97%|███▊| 242/250 [00:01<00:00, 133.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:19 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.777 | nll_loss 0.006 | accuracy 72.2 | wps 133720 | wpb 1045.3 | bsz 8 | num_updates 225\n",
      "2022-11-22 01:59:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-22 01:59:19 | INFO | train | epoch 001 | loss 0.925 | nll_loss 0.007 | accuracy 63.6 | wps 28606.9 | ups 26.76 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.0429e-06 | gnorm 3.942 | train_wall 7 | gb_free 9.4 | wall 9\n",
      "2022-11-22 01:59:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 002:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:59:19 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-22 01:59:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  99%|▉| 223/225 [00:05<00:00, 40.67it/s, loss=0.618, nll_loss=0.006, 2022-11-22 01:59:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 69.46it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 116.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.738 | nll_loss 0.006 | accuracy 70.5 | wps 135949 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 71.5\n",
      "2022-11-22 01:59:25 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 002 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 75.64it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 105.53it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  14%|▋    | 34/250 [00:00<00:01, 112.60it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  19%|▉    | 48/250 [00:00<00:01, 120.38it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  25%|█▏   | 62/250 [00:00<00:01, 125.43it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  30%|█▌   | 76/250 [00:00<00:01, 129.72it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  36%|█▊   | 90/250 [00:00<00:01, 131.11it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  42%|█▋  | 104/250 [00:00<00:01, 132.57it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  47%|█▉  | 118/250 [00:00<00:00, 132.81it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  53%|██  | 132/250 [00:01<00:00, 132.53it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  59%|██▎ | 147/250 [00:01<00:00, 135.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  64%|██▌ | 161/250 [00:01<00:00, 134.78it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  70%|██▊ | 175/250 [00:01<00:00, 134.29it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  76%|███ | 189/250 [00:01<00:00, 131.84it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  81%|███▏| 203/250 [00:01<00:00, 133.40it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  87%|███▍| 217/250 [00:01<00:00, 131.90it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  92%|███▋| 231/250 [00:01<00:00, 134.12it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  98%|███▉| 246/250 [00:01<00:00, 137.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:27 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.704 | nll_loss 0.005 | accuracy 76.7 | wps 137996 | wpb 1045.3 | bsz 8 | num_updates 450\n",
      "2022-11-22 01:59:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-22 01:59:27 | INFO | train | epoch 002 | loss 0.725 | nll_loss 0.005 | accuracy 76.4 | wps 29365.3 | ups 27.52 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.03217e-06 | gnorm 6.325 | train_wall 6 | gb_free 9.4 | wall 18\n",
      "2022-11-22 01:59:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 003:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:59:27 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-22 01:59:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 224/225 [00:06<00:00, 39.32it/s, loss=0.561, nll_loss=0.004, 2022-11-22 01:59:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 65.96it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 111.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:33 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.612 | nll_loss 0.005 | accuracy 82.5 | wps 133116 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 82.5\n",
      "2022-11-22 01:59:33 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 003 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   2%|▏      | 6/250 [00:00<00:04, 53.90it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   6%|▍     | 16/250 [00:00<00:03, 75.30it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  11%|▋     | 27/250 [00:00<00:02, 89.58it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  16%|▉     | 39/250 [00:00<00:02, 98.93it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  20%|█    | 51/250 [00:00<00:01, 104.50it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  25%|█▎   | 63/250 [00:00<00:01, 107.95it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  30%|█▌   | 76/250 [00:00<00:01, 114.17it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  35%|█▊   | 88/250 [00:00<00:01, 112.65it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  40%|█▌  | 100/250 [00:00<00:01, 110.56it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  45%|█▊  | 113/250 [00:01<00:01, 113.88it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  51%|██  | 127/250 [00:01<00:01, 119.56it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  56%|██▎ | 141/250 [00:01<00:00, 124.07it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  62%|██▍ | 154/250 [00:01<00:00, 124.29it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  67%|██▋ | 168/250 [00:01<00:00, 128.54it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  73%|██▉ | 182/250 [00:01<00:00, 129.10it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  79%|███▏| 197/250 [00:01<00:00, 133.55it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  84%|███▍| 211/250 [00:01<00:00, 130.99it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  90%|███▌| 225/250 [00:01<00:00, 122.12it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  95%|███▊| 238/250 [00:02<00:00, 122.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:35 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.644 | nll_loss 0.005 | accuracy 79.5 | wps 124445 | wpb 1045.3 | bsz 8 | num_updates 675\n",
      "2022-11-22 01:59:35 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-22 01:59:35 | INFO | train | epoch 003 | loss 0.64 | nll_loss 0.005 | accuracy 80.2 | wps 28337.3 | ups 26.55 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.02145e-06 | gnorm 7.419 | train_wall 6 | gb_free 9.5 | wall 26\n",
      "2022-11-22 01:59:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 004:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:59:35 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-22 01:59:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  98%|▉| 220/225 [00:06<00:00, 38.47it/s, loss=0.677, nll_loss=0.005, 2022-11-22 01:59:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 63.34it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  84%|████▏| 21/25 [00:00<00:00, 106.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:42 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.58 | nll_loss 0.004 | accuracy 84 | wps 125962 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 84\n",
      "2022-11-22 01:59:42 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 004 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 74.54it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 104.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 115.66it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  19%|▉    | 47/250 [00:00<00:01, 113.29it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  24%|█▏   | 59/250 [00:00<00:01, 112.74it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  28%|█▍   | 71/250 [00:00<00:01, 107.76it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  34%|█▋   | 84/250 [00:00<00:01, 113.09it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  39%|█▉   | 97/250 [00:00<00:01, 116.88it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  44%|█▊  | 110/250 [00:00<00:01, 119.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  49%|█▉  | 123/250 [00:01<00:01, 114.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  55%|██▏ | 137/250 [00:01<00:00, 118.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  60%|██▍ | 151/250 [00:01<00:00, 122.83it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  66%|██▌ | 164/250 [00:01<00:00, 120.66it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  71%|██▊ | 177/250 [00:01<00:00, 119.30it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  76%|███ | 191/250 [00:01<00:00, 122.45it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  82%|███▎| 204/250 [00:01<00:00, 121.59it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  87%|███▍| 217/250 [00:01<00:00, 118.19it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  92%|███▋| 229/250 [00:01<00:00, 116.99it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  97%|███▉| 243/250 [00:02<00:00, 120.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:44 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.64 | nll_loss 0.005 | accuracy 79.6 | wps 123998 | wpb 1045.3 | bsz 8 | num_updates 900\n",
      "2022-11-22 01:59:44 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-22 01:59:44 | INFO | train | epoch 004 | loss 0.57 | nll_loss 0.004 | accuracy 82.8 | wps 27152.5 | ups 25.44 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.01072e-06 | gnorm 8.035 | train_wall 6 | gb_free 9.4 | wall 35\n",
      "2022-11-22 01:59:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 005:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:59:44 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-22 01:59:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  98%|▉| 221/225 [00:05<00:00, 38.60it/s, loss=0.651, nll_loss=0.005, 2022-11-22 01:59:50 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
      "2022-11-22 01:59:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 67.61it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 118.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.581 | nll_loss 0.004 | accuracy 83 | wps 135396 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 84\n",
      "2022-11-22 01:59:50 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 005 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 78.25it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   9%|▍    | 22/250 [00:00<00:02, 109.10it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  14%|▋    | 36/250 [00:00<00:01, 120.41it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 119.57it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  24%|█▏   | 61/250 [00:00<00:01, 117.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  30%|█▌   | 75/250 [00:00<00:01, 122.78it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  36%|█▊   | 89/250 [00:00<00:01, 127.54it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  41%|█▋  | 102/250 [00:00<00:01, 128.08it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  46%|█▊  | 115/250 [00:00<00:01, 127.77it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  51%|██  | 128/250 [00:01<00:00, 126.47it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  57%|██▎ | 143/250 [00:01<00:00, 131.46it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  63%|██▌ | 157/250 [00:01<00:00, 132.07it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  68%|██▋ | 171/250 [00:01<00:00, 127.89it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  74%|██▉ | 184/250 [00:01<00:00, 128.36it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  79%|███▏| 197/250 [00:01<00:00, 128.10it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  84%|███▎| 210/250 [00:01<00:00, 126.83it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  90%|███▌| 224/250 [00:01<00:00, 129.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  95%|███▊| 238/250 [00:01<00:00, 131.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 01:59:52 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.634 | nll_loss 0.005 | accuracy 80.2 | wps 135118 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
      "2022-11-22 01:59:52 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-22 01:59:52 | INFO | train | epoch 005 | loss 0.549 | nll_loss 0.004 | accuracy 83.9 | wps 29146.7 | ups 27.31 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 8.257 | train_wall 6 | gb_free 9.4 | wall 43\n",
      "2022-11-22 01:59:52 | INFO | fairseq_cli.train | done training in 42.9 seconds\n"
     ]
    }
   ],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
    "                --restore-file $MODEL_PATH \\\n",
    "                --batch-size $MAX_SENTENCES \\\n",
    "                --task $TASK \\\n",
    "                --update-freq 1 \\\n",
    "                --seed $SEED \\\n",
    "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                --init-token 0 \\\n",
    "                --separator-token 2 \\\n",
    "                --arch roberta_small \\\n",
    "                --criterion sentence_prediction \\\n",
    "                --num-classes $NUM_CLASSES \\\n",
    "                --weight-decay 0.01 \\\n",
    "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                --maximize-best-checkpoint-metric \\\n",
    "                --best-checkpoint-metric $METRIC \\\n",
    "                --save-dir $SAVE_DIR \\\n",
    "                --lr-scheduler polynomial_decay \\\n",
    "                --lr $LR \\\n",
    "                --max-update $MAX_UPDATE \\\n",
    "                --total-num-update $MAX_UPDATE \\\n",
    "                --no-epoch-checkpoints \\\n",
    "                --no-last-checkpoints \\\n",
    "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
    "                --log-interval 5 \\\n",
    "                --warmup-updates $WARMUP \\\n",
    "                --max-epoch $MAX_EPOCH \\\n",
    "                --keep-best-checkpoints 1 \\\n",
    "                --max-positions 256 \\\n",
    "                --valid-subset $VALID_SUBSET \\\n",
    "                --shorten-method 'truncate' \\\n",
    "                --no-save \\\n",
    "                --distributed-world-size 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi1U19Uunnse"
   },
   "source": [
    "## <b>Random $RoBERTa_{small}^{fr}$ model training:</b>\n",
    "\n",
    "In this section you have to finetune a random checkpinf of the model $RoBERTa_{small}^{fr}$ using the same setting as before (<b>Hint:</b> an unexisted model path will not give you an error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lSLWP6VUhUlC"
   },
   "outputs": [],
   "source": [
    "DATA_SET = \"books\"\n",
    "TASK = \"sentence_prediction\"  # fill me, sentence prediction task on fairseq\n",
    "MODEL = \"RoBERTa_small_fr_random\"\n",
    "DATA_PATH = \"data/cls.books\"  # fill me\n",
    "MODEL_PATH = \"xxx\"  # fill me\n",
    "MAX_EPOCH = 5  # fill me\n",
    "MAX_SENTENCES = 8  # fill me, batch size\n",
    "MAX_UPDATE = int(\n",
    "    MAX_EPOCH * 1800 / MAX_SENTENCES\n",
    ")  # fill me, n_epochs * n_train_examples / total batch size\n",
    "LR = 1e-5  # fill me\n",
    "VALID_SUBSET = \"valid,test\"  # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
    "METRIC = \"accuracy\"  # fill me, use the accuracy metric\n",
    "NUM_CLASSES = 2  # fill me, number of classes\n",
    "SEEDS = 3\n",
    "CUDA_VISIBLE_DEVICES = 0\n",
    "WARMUP = 6  # fill me, warmup ratio=6% of the whole training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qtsCysc4hb42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', 'restore_file': 'xxx', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', restore_file='xxx', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls.books', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls.books', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 6, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-22 01:59:55 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2022-11-22 01:59:55 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2022-11-22 01:59:55 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-22 01:59:55 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/input0/valid\n",
      "2022-11-22 01:59:55 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/label/valid\n",
      "2022-11-22 01:59:55 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2022-11-22 01:59:55 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/input0/test\n",
      "2022-11-22 01:59:55 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/label/test\n",
      "2022-11-22 01:59:55 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2022-11-22 01:59:57 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2022-11-22 01:59:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:59:57 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 10.000 GB ; name = NVIDIA GeForce RTX 3080                 \n",
      "2022-11-22 01:59:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 01:59:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-22 01:59:57 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2022-11-22 01:59:57 | INFO | fairseq.trainer | Preparing to load checkpoint xxx\n",
      "2022-11-22 01:59:57 | INFO | fairseq.trainer | No existing checkpoint found xxx\n",
      "2022-11-22 01:59:57 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-22 01:59:57 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/input0/train\n",
      "2022-11-22 01:59:57 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/label/train\n",
      "2022-11-22 01:59:57 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2022-11-22 01:59:57 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-22 01:59:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 01:59:57 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-22 01:59:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001: 100%|▉| 224/225 [00:06<00:00, 37.13it/s, loss=0.957, nll_loss=0.007, 2022-11-22 02:00:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 74.33it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 114.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.998 | nll_loss 0.008 | accuracy 50 | wps 133513 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2022-11-22 02:00:04 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 77.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   9%|▍    | 22/250 [00:00<00:02, 110.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 119.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 125.09it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  26%|█▎   | 64/250 [00:00<00:01, 132.06it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  31%|█▌   | 78/250 [00:00<00:01, 134.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  37%|█▊   | 92/250 [00:00<00:01, 135.99it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  43%|█▋  | 107/250 [00:00<00:01, 137.43it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  48%|█▉  | 121/250 [00:00<00:00, 135.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  54%|██▏ | 135/250 [00:01<00:00, 136.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  60%|██▍ | 150/250 [00:01<00:00, 138.62it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  66%|██▋ | 165/250 [00:01<00:00, 139.10it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  72%|██▊ | 179/250 [00:01<00:00, 135.10it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  77%|███ | 193/250 [00:01<00:00, 132.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  83%|███▎| 207/250 [00:01<00:00, 130.47it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  88%|███▌| 221/250 [00:01<00:00, 131.73it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  94%|███▊| 235/250 [00:01<00:00, 134.09it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset: 100%|████| 250/250 [00:01<00:00, 136.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:06 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.993 | nll_loss 0.008 | accuracy 50.8 | wps 140503 | wpb 1045.3 | bsz 8 | num_updates 225\n",
      "2022-11-22 02:00:06 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-22 02:00:06 | INFO | train | epoch 001 | loss 1.004 | nll_loss 0.008 | accuracy 51.7 | wps 28913.1 | ups 27.08 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.0429e-06 | gnorm 4.604 | train_wall 7 | gb_free 9.4 | wall 9\n",
      "2022-11-22 02:00:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 002:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:00:06 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-22 02:00:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  98%|▉| 220/225 [00:06<00:00, 39.42it/s, loss=1.03, nll_loss=0.007, a2022-11-22 02:00:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 73.24it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 116.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.979 | nll_loss 0.007 | accuracy 60.5 | wps 132573 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 60.5\n",
      "2022-11-22 02:00:13 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 002 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   2%|▏      | 6/250 [00:00<00:04, 59.10it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   8%|▍    | 20/250 [00:00<00:02, 102.84it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 121.65it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 127.77it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  25%|█▎   | 63/250 [00:00<00:01, 129.29it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  31%|█▌   | 77/250 [00:00<00:01, 130.13it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  36%|█▊   | 91/250 [00:00<00:01, 130.73it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  42%|█▋  | 105/250 [00:00<00:01, 127.93it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  47%|█▉  | 118/250 [00:00<00:01, 127.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  53%|██▏ | 133/250 [00:01<00:00, 131.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  59%|██▎ | 147/250 [00:01<00:00, 129.12it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  64%|██▌ | 161/250 [00:01<00:00, 129.94it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  70%|██▊ | 175/250 [00:01<00:00, 131.54it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  76%|███ | 189/250 [00:01<00:00, 132.27it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  81%|███▏| 203/250 [00:01<00:00, 133.67it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  87%|███▍| 217/250 [00:01<00:00, 135.34it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  93%|███▋| 232/250 [00:01<00:00, 137.19it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  99%|███▉| 247/250 [00:01<00:00, 140.74it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:15 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.967 | nll_loss 0.007 | accuracy 62.6 | wps 138568 | wpb 1045.3 | bsz 8 | num_updates 450\n",
      "2022-11-22 02:00:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-22 02:00:15 | INFO | train | epoch 002 | loss 0.984 | nll_loss 0.007 | accuracy 57.2 | wps 28268.8 | ups 26.49 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.03217e-06 | gnorm 4.542 | train_wall 6 | gb_free 9.4 | wall 18\n",
      "2022-11-22 02:00:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 003:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:00:15 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-22 02:00:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 224/225 [00:06<00:00, 36.17it/s, loss=0.937, nll_loss=0.007, 2022-11-22 02:00:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 75.10it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  96%|████▊| 24/25 [00:00<00:00, 122.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.964 | nll_loss 0.007 | accuracy 60.5 | wps 139917 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 60.5\n",
      "2022-11-22 02:00:21 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 003 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 78.82it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 105.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  13%|▋    | 32/250 [00:00<00:02, 105.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  19%|▉    | 47/250 [00:00<00:01, 119.90it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  24%|█▏   | 59/250 [00:00<00:01, 119.27it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  28%|█▍   | 71/250 [00:00<00:01, 117.45it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  33%|█▋   | 83/250 [00:00<00:01, 113.51it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  38%|█▉   | 95/250 [00:00<00:01, 115.25it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  43%|█▋  | 107/250 [00:00<00:01, 115.33it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  48%|█▉  | 119/250 [00:01<00:01, 116.20it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  53%|██  | 132/250 [00:01<00:00, 118.60it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  58%|██▎ | 144/250 [00:01<00:00, 116.93it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  63%|██▌ | 158/250 [00:01<00:00, 122.27it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  68%|██▋ | 171/250 [00:01<00:00, 120.71it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  74%|██▉ | 186/250 [00:01<00:00, 127.58it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  80%|███▏| 200/250 [00:01<00:00, 130.46it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  86%|███▍| 214/250 [00:01<00:00, 125.16it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  91%|███▋| 227/250 [00:01<00:00, 125.62it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  96%|███▊| 240/250 [00:02<00:00, 124.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:23 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.944 | nll_loss 0.007 | accuracy 66.2 | wps 127788 | wpb 1045.3 | bsz 8 | num_updates 675\n",
      "2022-11-22 02:00:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-22 02:00:23 | INFO | train | epoch 003 | loss 0.959 | nll_loss 0.007 | accuracy 60.1 | wps 28016.1 | ups 26.25 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.02145e-06 | gnorm 4.611 | train_wall 6 | gb_free 9.4 | wall 26\n",
      "2022-11-22 02:00:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 004:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:00:23 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-22 02:00:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  99%|▉| 223/225 [00:05<00:00, 41.52it/s, loss=0.878, nll_loss=0.006, 2022-11-22 02:00:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:00<00:00, 74.53it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:29 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.948 | nll_loss 0.007 | accuracy 61.5 | wps 143852 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 61.5\n",
      "2022-11-22 02:00:29 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 004 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   4%|▎      | 9/250 [00:00<00:02, 83.44it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   8%|▍    | 21/250 [00:00<00:02, 100.76it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 117.89it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  20%|█    | 50/250 [00:00<00:01, 128.20it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  25%|█▎   | 63/250 [00:00<00:01, 125.73it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  31%|█▌   | 77/250 [00:00<00:01, 129.32it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  36%|█▊   | 90/250 [00:00<00:01, 129.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  41%|█▋  | 103/250 [00:00<00:01, 125.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  46%|█▊  | 116/250 [00:00<00:01, 118.40it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  52%|██  | 130/250 [00:01<00:00, 123.54it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  58%|██▎ | 144/250 [00:01<00:00, 126.63it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  63%|██▌ | 157/250 [00:01<00:00, 117.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  68%|██▋ | 169/250 [00:01<00:00, 117.68it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  73%|██▉ | 183/250 [00:01<00:00, 123.64it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  79%|███▏| 198/250 [00:01<00:00, 129.39it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  85%|███▍| 212/250 [00:01<00:00, 131.52it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  90%|███▌| 226/250 [00:01<00:00, 132.56it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  96%|███▊| 240/250 [00:01<00:00, 134.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:31 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.921 | nll_loss 0.007 | accuracy 65.5 | wps 133606 | wpb 1045.3 | bsz 8 | num_updates 900\n",
      "2022-11-22 02:00:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-22 02:00:31 | INFO | train | epoch 004 | loss 0.924 | nll_loss 0.007 | accuracy 66.9 | wps 29197.9 | ups 27.36 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.01072e-06 | gnorm 4.727 | train_wall 6 | gb_free 9.4 | wall 34\n",
      "2022-11-22 02:00:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 005:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:00:31 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-22 02:00:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  99%|▉| 222/225 [00:06<00:00, 37.48it/s, loss=0.894, nll_loss=0.006, 2022-11-22 02:00:38 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
      "2022-11-22 02:00:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 67.75it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 120.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:38 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.941 | nll_loss 0.007 | accuracy 61 | wps 135935 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 61.5\n",
      "2022-11-22 02:00:38 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 005 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 74.22it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   9%|▍    | 22/250 [00:00<00:02, 106.89it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 116.99it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 124.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  25%|█▏   | 62/250 [00:00<00:01, 117.87it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  30%|█▍   | 74/250 [00:00<00:01, 112.80it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  34%|█▋   | 86/250 [00:00<00:01, 111.80it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  40%|█▌  | 100/250 [00:00<00:01, 119.28it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  45%|█▊  | 113/250 [00:00<00:01, 122.18it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  51%|██  | 127/250 [00:01<00:00, 126.31it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  56%|██▎ | 141/250 [00:01<00:00, 129.06it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  62%|██▍ | 155/250 [00:01<00:00, 130.15it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  68%|██▋ | 169/250 [00:01<00:00, 123.82it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  73%|██▉ | 183/250 [00:01<00:00, 127.72it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  79%|███▏| 198/250 [00:01<00:00, 131.40it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  85%|███▍| 212/250 [00:01<00:00, 132.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  90%|███▌| 226/250 [00:01<00:00, 128.51it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  96%|███▊| 239/250 [00:01<00:00, 128.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:40 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.911 | nll_loss 0.007 | accuracy 67.2 | wps 132348 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
      "2022-11-22 02:00:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-22 02:00:40 | INFO | train | epoch 005 | loss 0.898 | nll_loss 0.007 | accuracy 68.3 | wps 28084.8 | ups 26.32 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 4.719 | train_wall 6 | gb_free 9.5 | wall 43\n",
      "2022-11-22 02:00:40 | INFO | fairseq_cli.train | done training in 43.0 seconds\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', 'restore_file': 'xxx', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', restore_file='xxx', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls.books', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls.books', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 6, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-22 02:00:43 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2022-11-22 02:00:43 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2022-11-22 02:00:43 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-22 02:00:43 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/input0/valid\n",
      "2022-11-22 02:00:43 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/label/valid\n",
      "2022-11-22 02:00:43 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2022-11-22 02:00:43 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/input0/test\n",
      "2022-11-22 02:00:43 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/label/test\n",
      "2022-11-22 02:00:43 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2022-11-22 02:00:44 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2022-11-22 02:00:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 02:00:44 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 10.000 GB ; name = NVIDIA GeForce RTX 3080                 \n",
      "2022-11-22 02:00:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 02:00:44 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-22 02:00:44 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2022-11-22 02:00:44 | INFO | fairseq.trainer | Preparing to load checkpoint xxx\n",
      "2022-11-22 02:00:44 | INFO | fairseq.trainer | No existing checkpoint found xxx\n",
      "2022-11-22 02:00:44 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-22 02:00:44 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/input0/train\n",
      "2022-11-22 02:00:44 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/label/train\n",
      "2022-11-22 02:00:44 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2022-11-22 02:00:44 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-22 02:00:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:00:44 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-22 02:00:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001: 100%|▉| 224/225 [00:06<00:00, 37.34it/s, loss=0.994, nll_loss=0.008, 2022-11-22 02:00:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 64.62it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|████▍| 22/25 [00:00<00:00, 113.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:51 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.993 | nll_loss 0.008 | accuracy 53 | wps 134272 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2022-11-22 02:00:51 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 69.00it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   7%|▍     | 18/250 [00:00<00:02, 89.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  12%|▋     | 30/250 [00:00<00:02, 99.75it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  17%|▊    | 43/250 [00:00<00:01, 109.33it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  22%|█    | 56/250 [00:00<00:01, 113.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  27%|█▎   | 68/250 [00:00<00:01, 113.27it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  32%|█▌   | 80/250 [00:00<00:01, 108.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  36%|█▊   | 91/250 [00:00<00:01, 105.18it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  41%|█▋  | 103/250 [00:00<00:01, 108.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  47%|█▊  | 117/250 [00:01<00:01, 115.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  52%|██  | 131/250 [00:01<00:00, 120.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  58%|██▎ | 144/250 [00:01<00:00, 121.47it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  63%|██▌ | 157/250 [00:01<00:00, 122.93it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  68%|██▋ | 170/250 [00:01<00:00, 124.42it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  73%|██▉ | 183/250 [00:01<00:00, 124.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  78%|███▏| 196/250 [00:01<00:00, 125.02it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  84%|███▎| 209/250 [00:01<00:00, 125.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  89%|███▌| 222/250 [00:01<00:00, 124.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  94%|███▊| 235/250 [00:02<00:00, 124.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:53 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.986 | nll_loss 0.008 | accuracy 55.1 | wps 125469 | wpb 1045.3 | bsz 8 | num_updates 225\n",
      "2022-11-22 02:00:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-22 02:00:53 | INFO | train | epoch 001 | loss 1.001 | nll_loss 0.008 | accuracy 50.7 | wps 28072.9 | ups 26.34 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.0429e-06 | gnorm 4.539 | train_wall 7 | gb_free 9.4 | wall 9\n",
      "2022-11-22 02:00:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 002:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:00:53 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-22 02:00:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  98%|▉| 221/225 [00:05<00:00, 38.85it/s, loss=0.954, nll_loss=0.008, 2022-11-22 02:00:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  24%|█▋     | 6/25 [00:00<00:00, 59.73it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  76%|███▊ | 19/25 [00:00<00:00, 100.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:00:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.978 | nll_loss 0.007 | accuracy 59.5 | wps 125714 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 59.5\n",
      "2022-11-22 02:00:59 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 002 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   2%|▏      | 6/250 [00:00<00:04, 55.95it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   8%|▍     | 19/250 [00:00<00:02, 96.34it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  13%|▋    | 32/250 [00:00<00:01, 110.80it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  18%|▉    | 45/250 [00:00<00:01, 118.05it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  23%|█▏   | 57/250 [00:00<00:01, 116.92it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  28%|█▍   | 70/250 [00:00<00:01, 119.81it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  34%|█▋   | 84/250 [00:00<00:01, 125.00it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  39%|█▉   | 98/250 [00:00<00:01, 127.23it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  44%|█▊  | 111/250 [00:00<00:01, 124.19it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  50%|█▉  | 124/250 [00:01<00:01, 124.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  55%|██▏ | 137/250 [00:01<00:00, 125.15it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  60%|██▍ | 150/250 [00:01<00:00, 124.30it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  66%|██▌ | 164/250 [00:01<00:00, 127.44it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  71%|██▊ | 177/250 [00:01<00:00, 126.34it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  76%|███ | 190/250 [00:01<00:00, 122.21it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  81%|███▏| 203/250 [00:01<00:00, 122.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  87%|███▍| 217/250 [00:01<00:00, 124.91it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  92%|███▋| 231/250 [00:01<00:00, 126.99it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  98%|███▉| 246/250 [00:01<00:00, 132.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:02 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.964 | nll_loss 0.007 | accuracy 64.9 | wps 131768 | wpb 1045.3 | bsz 8 | num_updates 450\n",
      "2022-11-22 02:01:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-22 02:01:02 | INFO | train | epoch 002 | loss 0.98 | nll_loss 0.007 | accuracy 55.5 | wps 29345.6 | ups 27.5 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.03217e-06 | gnorm 4.623 | train_wall 6 | gb_free 9.4 | wall 17\n",
      "2022-11-22 02:01:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 003:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:02 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-22 02:01:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  99%|▉| 223/225 [00:05<00:00, 37.05it/s, loss=0.889, nll_loss=0.006, 2022-11-22 02:01:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  20%|█▍     | 5/25 [00:00<00:00, 49.28it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  64%|███▊  | 16/25 [00:00<00:00, 83.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:08 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.958 | nll_loss 0.007 | accuracy 65.5 | wps 116862 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 65.5\n",
      "2022-11-22 02:01:08 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 003 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   7%|▍     | 18/250 [00:00<00:02, 91.39it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  13%|▋    | 32/250 [00:00<00:01, 111.14it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  18%|▉    | 45/250 [00:00<00:01, 117.91it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  23%|█▏   | 58/250 [00:00<00:01, 119.96it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  29%|█▍   | 72/250 [00:00<00:01, 126.06it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  34%|█▋   | 85/250 [00:00<00:01, 126.30it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  39%|█▉   | 98/250 [00:00<00:01, 124.73it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  45%|█▊  | 113/250 [00:00<00:01, 129.69it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  51%|██  | 127/250 [00:01<00:00, 129.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  56%|██▎ | 141/250 [00:01<00:00, 132.31it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  62%|██▍ | 155/250 [00:01<00:00, 124.22it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  68%|██▋ | 170/250 [00:01<00:00, 130.08it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  74%|██▉ | 184/250 [00:01<00:00, 128.94it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  79%|███▏| 197/250 [00:01<00:00, 123.71it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  85%|███▍| 212/250 [00:01<00:00, 129.36it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  91%|███▋| 227/250 [00:01<00:00, 132.63it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  96%|███▊| 241/250 [00:01<00:00, 129.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:10 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.934 | nll_loss 0.007 | accuracy 68.8 | wps 133947 | wpb 1045.3 | bsz 8 | num_updates 675\n",
      "2022-11-22 02:01:10 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-22 02:01:10 | INFO | train | epoch 003 | loss 0.946 | nll_loss 0.007 | accuracy 64.2 | wps 29141.8 | ups 27.31 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.02145e-06 | gnorm 4.622 | train_wall 6 | gb_free 9.5 | wall 26\n",
      "2022-11-22 02:01:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 004:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:10 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-22 02:01:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  98%|▉| 220/225 [00:05<00:00, 40.69it/s, loss=0.818, nll_loss=0.007, 2022-11-22 02:01:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  24%|█▋     | 6/25 [00:00<00:00, 56.05it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  76%|████▌ | 19/25 [00:00<00:00, 96.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.95 | nll_loss 0.007 | accuracy 58.5 | wps 118362 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 65.5\n",
      "2022-11-22 02:01:16 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 004 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   2%|▏      | 6/250 [00:00<00:04, 59.59it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   8%|▍     | 19/250 [00:00<00:02, 97.62it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  13%|▋    | 33/250 [00:00<00:01, 113.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  18%|▉    | 46/250 [00:00<00:01, 118.89it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  24%|█▏   | 60/250 [00:00<00:01, 125.87it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  30%|█▍   | 74/250 [00:00<00:01, 129.06it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  35%|█▊   | 88/250 [00:00<00:01, 130.02it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  41%|█▋  | 102/250 [00:00<00:01, 131.68it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  46%|█▊  | 116/250 [00:00<00:01, 130.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  52%|██  | 130/250 [00:01<00:00, 129.65it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  57%|██▎ | 143/250 [00:01<00:00, 117.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  63%|██▌ | 157/250 [00:01<00:00, 121.66it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  68%|██▋ | 171/250 [00:01<00:00, 125.95it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  74%|██▉ | 185/250 [00:01<00:00, 129.35it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  80%|███▏| 199/250 [00:01<00:00, 130.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  85%|███▍| 213/250 [00:01<00:00, 126.33it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  90%|███▌| 226/250 [00:01<00:00, 125.33it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  96%|███▊| 239/250 [00:01<00:00, 126.36it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:18 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.917 | nll_loss 0.007 | accuracy 61.9 | wps 132693 | wpb 1045.3 | bsz 8 | num_updates 900\n",
      "2022-11-22 02:01:18 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-22 02:01:18 | INFO | train | epoch 004 | loss 0.907 | nll_loss 0.007 | accuracy 68.2 | wps 29549.3 | ups 27.69 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.01072e-06 | gnorm 4.769 | train_wall 5 | gb_free 9.4 | wall 34\n",
      "2022-11-22 02:01:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 005:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:18 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-22 02:01:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  99%|▉| 223/225 [00:05<00:00, 40.61it/s, loss=0.808, nll_loss=0.007, 2022-11-22 02:01:24 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
      "2022-11-22 02:01:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  24%|█▋     | 6/25 [00:00<00:00, 58.19it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  76%|████▌ | 19/25 [00:00<00:00, 99.68it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:24 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.93 | nll_loss 0.007 | accuracy 63.5 | wps 123670 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 65.5\n",
      "2022-11-22 02:01:24 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 005 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 76.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   9%|▍    | 22/250 [00:00<00:02, 112.38it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  14%|▋    | 36/250 [00:00<00:01, 122.86it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  20%|█    | 50/250 [00:00<00:01, 128.75it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  26%|█▎   | 64/250 [00:00<00:01, 131.62it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  31%|█▌   | 78/250 [00:00<00:01, 133.58it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  37%|█▊   | 92/250 [00:00<00:01, 131.92it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  42%|█▋  | 106/250 [00:00<00:01, 126.31it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  48%|█▉  | 120/250 [00:00<00:01, 128.09it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  54%|██▏ | 134/250 [00:01<00:00, 129.07it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  59%|██▎ | 148/250 [00:01<00:00, 129.20it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  64%|██▌ | 161/250 [00:01<00:00, 127.27it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  70%|██▊ | 175/250 [00:01<00:00, 129.93it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  76%|███ | 189/250 [00:01<00:00, 130.43it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  81%|███▏| 203/250 [00:01<00:00, 132.78it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  87%|███▍| 217/250 [00:01<00:00, 133.18it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  92%|███▋| 231/250 [00:01<00:00, 131.42it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  98%|███▉| 245/250 [00:01<00:00, 133.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:26 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.896 | nll_loss 0.007 | accuracy 68.3 | wps 137294 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
      "2022-11-22 02:01:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-22 02:01:26 | INFO | train | epoch 005 | loss 0.883 | nll_loss 0.007 | accuracy 70.2 | wps 30028.1 | ups 28.14 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 4.957 | train_wall 5 | gb_free 9.4 | wall 42\n",
      "2022-11-22 02:01:26 | INFO | fairseq_cli.train | done training in 41.9 seconds\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', 'restore_file': 'xxx', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', restore_file='xxx', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls.books', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=6, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls.books', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 6, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-22 02:01:29 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2022-11-22 02:01:29 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2022-11-22 02:01:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-22 02:01:29 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/input0/valid\n",
      "2022-11-22 02:01:29 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls.books/label/valid\n",
      "2022-11-22 02:01:29 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2022-11-22 02:01:29 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/input0/test\n",
      "2022-11-22 02:01:29 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls.books/label/test\n",
      "2022-11-22 02:01:29 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2022-11-22 02:01:30 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2022-11-22 02:01:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 02:01:30 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 10.000 GB ; name = NVIDIA GeForce RTX 3080                 \n",
      "2022-11-22 02:01:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-22 02:01:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-22 02:01:30 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2022-11-22 02:01:30 | INFO | fairseq.trainer | Preparing to load checkpoint xxx\n",
      "2022-11-22 02:01:30 | INFO | fairseq.trainer | No existing checkpoint found xxx\n",
      "2022-11-22 02:01:30 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-22 02:01:30 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/input0/train\n",
      "2022-11-22 02:01:30 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls.books/label/train\n",
      "2022-11-22 02:01:30 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2022-11-22 02:01:30 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2022-11-22 02:01:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:30 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-22 02:01:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001:  99%|▉| 223/225 [00:06<00:00, 39.85it/s, loss=0.994, nll_loss=0.007, 2022-11-22 02:01:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 65.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  84%|████▏| 21/25 [00:00<00:00, 106.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.996 | nll_loss 0.008 | accuracy 50 | wps 125626 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2022-11-22 02:01:37 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 8/250 [00:00<00:03, 75.03it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   9%|▍    | 22/250 [00:00<00:02, 110.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▋    | 34/250 [00:00<00:01, 108.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  19%|▉    | 47/250 [00:00<00:01, 115.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  24%|█▏   | 61/250 [00:00<00:01, 121.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  30%|█▌   | 75/250 [00:00<00:01, 124.52it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  36%|█▊   | 89/250 [00:00<00:01, 127.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  41%|█▋  | 103/250 [00:00<00:01, 130.08it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  47%|█▊  | 117/250 [00:00<00:01, 127.55it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  52%|██  | 130/250 [00:01<00:00, 126.54it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  57%|██▎ | 143/250 [00:01<00:00, 124.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  62%|██▍ | 156/250 [00:01<00:00, 125.90it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  68%|██▋ | 170/250 [00:01<00:00, 128.03it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  74%|██▉ | 184/250 [00:01<00:00, 127.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  79%|███▏| 197/250 [00:01<00:00, 126.54it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  84%|███▍| 211/250 [00:01<00:00, 129.13it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  90%|███▌| 224/250 [00:01<00:00, 129.34it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  95%|███▊| 237/250 [00:01<00:00, 126.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:39 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.994 | nll_loss 0.008 | accuracy 50.1 | wps 133331 | wpb 1045.3 | bsz 8 | num_updates 225\n",
      "2022-11-22 02:01:39 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-22 02:01:39 | INFO | train | epoch 001 | loss 1.002 | nll_loss 0.008 | accuracy 51.8 | wps 29193.5 | ups 27.31 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.0429e-06 | gnorm 4.579 | train_wall 6 | gb_free 9.4 | wall 9\n",
      "2022-11-22 02:01:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 002:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:39 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-22 02:01:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  99%|▉| 222/225 [00:06<00:00, 38.83it/s, loss=1.023, nll_loss=0.01, a2022-11-22 02:01:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 65.55it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  84%|████▏| 21/25 [00:00<00:00, 107.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.981 | nll_loss 0.007 | accuracy 54.5 | wps 127575 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 54.5\n",
      "2022-11-22 02:01:45 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 002 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 65.30it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:   8%|▍    | 20/250 [00:00<00:02, 101.41it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  14%|▋    | 34/250 [00:00<00:01, 116.92it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  20%|▉    | 49/250 [00:00<00:01, 128.11it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  25%|█▎   | 63/250 [00:00<00:01, 129.77it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  30%|█▌   | 76/250 [00:00<00:01, 124.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  36%|█▊   | 91/250 [00:00<00:01, 129.55it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  42%|█▋  | 105/250 [00:00<00:01, 132.36it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  48%|█▉  | 119/250 [00:00<00:01, 124.76it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  54%|██▏ | 134/250 [00:01<00:00, 129.30it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  60%|██▍ | 149/250 [00:01<00:00, 133.23it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  65%|██▌ | 163/250 [00:01<00:00, 134.99it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  71%|██▊ | 177/250 [00:01<00:00, 131.15it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  76%|███ | 191/250 [00:01<00:00, 133.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  82%|███▎| 205/250 [00:01<00:00, 133.29it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  88%|███▌| 219/250 [00:01<00:00, 125.59it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset:  94%|███▋| 234/250 [00:01<00:00, 130.36it/s]\u001b[A\n",
      "epoch 002 | valid on 'test' subset: 100%|███▉| 249/250 [00:01<00:00, 135.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:47 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.975 | nll_loss 0.007 | accuracy 55.6 | wps 137212 | wpb 1045.3 | bsz 8 | num_updates 450\n",
      "2022-11-22 02:01:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-22 02:01:47 | INFO | train | epoch 002 | loss 0.987 | nll_loss 0.007 | accuracy 57.3 | wps 29015.9 | ups 27.19 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.03217e-06 | gnorm 4.562 | train_wall 6 | gb_free 9.4 | wall 17\n",
      "2022-11-22 02:01:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 003:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:47 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-22 02:01:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  98%|▉| 220/225 [00:05<00:00, 40.70it/s, loss=0.939, nll_loss=0.006, 2022-11-22 02:01:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  24%|█▋     | 6/25 [00:00<00:00, 57.47it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  80%|████ | 20/25 [00:00<00:00, 102.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.958 | nll_loss 0.007 | accuracy 63 | wps 127117 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 63\n",
      "2022-11-22 02:01:53 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 003 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   2%|▏      | 6/250 [00:00<00:04, 56.28it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:   7%|▍     | 17/250 [00:00<00:02, 85.63it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  12%|▌    | 30/250 [00:00<00:02, 104.25it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  17%|▊    | 42/250 [00:00<00:01, 109.83it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  23%|█▏   | 57/250 [00:00<00:01, 121.21it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  29%|█▍   | 72/250 [00:00<00:01, 128.77it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  34%|█▋   | 86/250 [00:00<00:01, 130.37it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  40%|█▌  | 101/250 [00:00<00:01, 135.61it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  46%|█▊  | 115/250 [00:00<00:00, 136.22it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  52%|██  | 130/250 [00:01<00:00, 138.56it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  58%|██▎ | 144/250 [00:01<00:00, 138.38it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  63%|██▌ | 158/250 [00:01<00:00, 138.31it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  69%|██▊ | 172/250 [00:01<00:00, 135.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  74%|██▉ | 186/250 [00:01<00:00, 130.74it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  80%|███▏| 201/250 [00:01<00:00, 135.51it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  86%|███▍| 215/250 [00:01<00:00, 136.48it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  92%|███▋| 229/250 [00:01<00:00, 136.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'test' subset:  97%|███▉| 243/250 [00:01<00:00, 136.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:01:55 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.945 | nll_loss 0.007 | accuracy 64.2 | wps 137732 | wpb 1045.3 | bsz 8 | num_updates 675\n",
      "2022-11-22 02:01:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-22 02:01:55 | INFO | train | epoch 003 | loss 0.96 | nll_loss 0.007 | accuracy 61.2 | wps 29836.5 | ups 27.96 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.02145e-06 | gnorm 4.625 | train_wall 5 | gb_free 9.5 | wall 25\n",
      "2022-11-22 02:01:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 004:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:01:55 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-22 02:01:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  99%|▉| 222/225 [00:05<00:00, 36.68it/s, loss=0.936, nll_loss=0.006, 2022-11-22 02:02:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  16%|█      | 4/25 [00:00<00:00, 37.14it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  64%|███▊  | 16/25 [00:00<00:00, 84.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:02:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.938 | nll_loss 0.007 | accuracy 62.5 | wps 119918 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 63\n",
      "2022-11-22 02:02:02 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 004 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   2%|▏      | 6/250 [00:00<00:04, 58.67it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:   8%|▍    | 20/250 [00:00<00:02, 104.22it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  14%|▋    | 35/250 [00:00<00:01, 121.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  19%|▉    | 48/250 [00:00<00:01, 115.42it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  25%|█▏   | 62/250 [00:00<00:01, 123.62it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  31%|█▌   | 77/250 [00:00<00:01, 129.65it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  36%|█▊   | 91/250 [00:00<00:01, 132.72it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  42%|█▋  | 105/250 [00:00<00:01, 134.27it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  48%|█▉  | 119/250 [00:00<00:01, 125.31it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  54%|██▏ | 134/250 [00:01<00:00, 130.30it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  60%|██▍ | 149/250 [00:01<00:00, 133.84it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  65%|██▌ | 163/250 [00:01<00:00, 132.37it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  71%|██▊ | 178/250 [00:01<00:00, 137.20it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  77%|███ | 193/250 [00:01<00:00, 138.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  83%|███▎| 207/250 [00:01<00:00, 137.27it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  88%|███▌| 221/250 [00:01<00:00, 127.08it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset:  94%|███▋| 234/250 [00:01<00:00, 127.59it/s]\u001b[A\n",
      "epoch 004 | valid on 'test' subset: 100%|███▉| 249/250 [00:01<00:00, 131.90it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:02:04 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.922 | nll_loss 0.007 | accuracy 68.2 | wps 136407 | wpb 1045.3 | bsz 8 | num_updates 900\n",
      "2022-11-22 02:02:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-22 02:02:04 | INFO | train | epoch 004 | loss 0.924 | nll_loss 0.007 | accuracy 67.2 | wps 29263.1 | ups 27.42 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.01072e-06 | gnorm 4.676 | train_wall 6 | gb_free 9.4 | wall 33\n",
      "2022-11-22 02:02:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 005:   0%|                                        | 0/225 [00:00<?, ?it/s]2022-11-22 02:02:04 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-22 02:02:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  98%|▉| 221/225 [00:06<00:00, 38.77it/s, loss=0.913, nll_loss=0.007, 2022-11-22 02:02:10 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
      "2022-11-22 02:02:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:00<00:00, 64.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|████▌| 23/25 [00:00<00:00, 118.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:02:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.931 | nll_loss 0.007 | accuracy 63 | wps 134444 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 63\n",
      "2022-11-22 02:02:10 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 005 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   3%|▏      | 7/250 [00:00<00:03, 68.56it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:   8%|▍    | 20/250 [00:00<00:02, 103.42it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  12%|▌    | 31/250 [00:00<00:02, 105.54it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  17%|▊    | 42/250 [00:00<00:01, 105.04it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  21%|█    | 53/250 [00:00<00:01, 101.00it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  26%|█▎   | 64/250 [00:00<00:01, 101.55it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  30%|█▌   | 76/250 [00:00<00:01, 105.94it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  36%|█▊   | 89/250 [00:00<00:01, 113.07it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  41%|█▋  | 102/250 [00:00<00:01, 116.51it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  46%|█▊  | 115/250 [00:01<00:01, 118.97it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  51%|██  | 128/250 [00:01<00:01, 121.15it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  56%|██▎ | 141/250 [00:01<00:00, 120.82it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  62%|██▍ | 154/250 [00:01<00:00, 119.40it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  67%|██▋ | 168/250 [00:01<00:00, 124.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  72%|██▉ | 181/250 [00:01<00:00, 121.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  78%|███ | 194/250 [00:01<00:00, 118.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  83%|███▎| 208/250 [00:01<00:00, 122.14it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  88%|███▌| 221/250 [00:01<00:00, 123.35it/s]\u001b[A\n",
      "epoch 005 | valid on 'test' subset:  94%|███▊| 235/250 [00:02<00:00, 127.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-22 02:02:12 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.912 | nll_loss 0.007 | accuracy 66.6 | wps 125104 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
      "2022-11-22 02:02:12 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-22 02:02:12 | INFO | train | epoch 005 | loss 0.901 | nll_loss 0.007 | accuracy 69.2 | wps 28067.5 | ups 26.3 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 4.751 | train_wall 6 | gb_free 9.4 | wall 42\n",
      "2022-11-22 02:02:12 | INFO | fairseq_cli.train | done training in 42.0 seconds\n"
     ]
    }
   ],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
    "                --restore-file $MODEL_PATH \\\n",
    "                --batch-size $MAX_SENTENCES \\\n",
    "                --task $TASK \\\n",
    "                --update-freq 1 \\\n",
    "                --seed $SEED \\\n",
    "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                --init-token 0 \\\n",
    "                --separator-token 2 \\\n",
    "                --arch roberta_small \\\n",
    "                --criterion sentence_prediction \\\n",
    "                --num-classes $NUM_CLASSES \\\n",
    "                --weight-decay 0.01 \\\n",
    "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                --maximize-best-checkpoint-metric \\\n",
    "                --best-checkpoint-metric $METRIC \\\n",
    "                --save-dir $SAVE_DIR \\\n",
    "                --lr-scheduler polynomial_decay \\\n",
    "                --lr $LR \\\n",
    "                --max-update $MAX_UPDATE \\\n",
    "                --total-num-update $MAX_UPDATE \\\n",
    "                --no-epoch-checkpoints \\\n",
    "                --no-last-checkpoints \\\n",
    "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
    "                --log-interval 5 \\\n",
    "                --warmup-updates $WARMUP \\\n",
    "                --max-epoch $MAX_EPOCH \\\n",
    "                --keep-best-checkpoints 1 \\\n",
    "                --max-positions 256 \\\n",
    "                --valid-subset $VALID_SUBSET \\\n",
    "                --shorten-method 'truncate' \\\n",
    "                --no-save \\\n",
    "                --distributed-world-size 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHACXaPSLwu4"
   },
   "source": [
    "## <b>Tensorboard Visualisation </B>\n",
    "\n",
    "In the this we will use tensorboard to visualize the training, validation and test accuracies. <b>Include and analyse in you report a screenshot of the test accuracy of the six models</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwVvJNExS2dl"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAR_P343MCKC"
   },
   "source": [
    "# <b>Part 2: HuggingFace's Transfromers</b>\n",
    "\n",
    "In this part of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset. Like in the first part we will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning). Again, if you are using you personal computer, do not run the following cell and use its content to download the files on you computer, since - depending on you operating system - running this cell will produce errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2ni-Bbql-CX"
   },
   "outputs": [],
   "source": [
    "%cd models\n",
    "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267607&authkey=APJub1wVzVLAoR8\" -O \"model_huggingface.zip\"\n",
    "!unzip model_huggingface.zip\n",
    "!rm model_huggingface.zip\n",
    "!rm -rf __MACOSX/\n",
    "\n",
    "%cd ../data\n",
    "!mkdir cls.books-json\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3M090L45oPn"
   },
   "source": [
    "## <b>Converting the CLS_Books dataset to json line files</b>\n",
    "\n",
    "Unlike Fairseq, you do not need to perform tokenization and binarization in Hugging Face transformer library. However, in order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test)\n",
    "\n",
    "for instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
    "\n",
    "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "<i>\n",
    "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
    "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
    "</i>\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HZZFHEHFyv5F"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SPLITS = [\"train\", \"test\", \"valid\"]\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(\"data/cls.books/\" + split + \".review\", \"r\") as f:\n",
    "        reviews = f.readlines()\n",
    "    with open(\"data/cls.books/\" + split + \".label\", \"r\") as f:\n",
    "        labels = f.readlines()\n",
    "    with open(\"data/cls.books-json/\" + split + \".json\", \"w\") as f:\n",
    "        # fill the gap here to create train.json, valid.json and test.json\n",
    "        for i, review in enumerate(reviews):\n",
    "            dico = {\"sentence1\": review[:-1], \"label\": labels[i][:-1]}\n",
    "            json.dump(dico, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICnN2FvnhTbs"
   },
   "source": [
    "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
    "\n",
    "In order to finrtune the model using HuggingFace, you to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. Make sure to use the same hyperparameter as in the first part of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "h-BBIykNjH7A"
   },
   "outputs": [],
   "source": [
    "DATA_SET = \"books\"\n",
    "TRAIN_FILE = \"data/cls.books-json/train.json\"\n",
    "VALIDATION_FILE = \"data/cls.books-json/valid.json\"\n",
    "TEST_FILE = \"data/cls.books-json/test.json\"\n",
    "MODEL = \"RoBERTa_small_fr_huggingface\"\n",
    "MODEL_PATH = \"models/RoBERTa_small_fr_HuggingFace/config.json\"\n",
    "MODEL_PATH = \"models/RoBERTa_small_fr_HuggingFace\"\n",
    "MAX_SENTENCES = 8  # fill me, batch size.\n",
    "LR = 1e-5  # fill me, learning rate\n",
    "MAX_EPOCH = 5  # fill me\n",
    "NUM_CLASSES = 2  # fill me\n",
    "SEEDS = 3\n",
    "CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lV2Zla33hK_C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/22/2022 02:02:25 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/22/2022 02:02:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.98,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/runs/Nov22_02-02-24_AROZYM,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=polynomial,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=0,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/22/2022 02:02:25 - INFO - __main__ - load a local file for train: data/cls.books-json/train.json\n",
      "11/22/2022 02:02:25 - INFO - __main__ - load a local file for validation: data/cls.books-json/valid.json\n",
      "11/22/2022 02:02:25 - INFO - __main__ - load a local file for test: data/cls.books-json/test.json\n",
      "11/22/2022 02:02:25 - WARNING - datasets.builder - Using custom data configuration default-403939cb3edeb7bb\n",
      "11/22/2022 02:02:25 - INFO - datasets.info - Loading Dataset Infos from /home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "11/22/2022 02:02:25 - INFO - datasets.builder - Generating dataset json (/home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "Downloading and preparing dataset json/default to /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
      "Downloading data files: 100%|██████████████████| 3/3 [00:00<00:00, 21845.33it/s]\n",
      "11/22/2022 02:02:25 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "11/22/2022 02:02:25 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 2911.36it/s]\n",
      "11/22/2022 02:02:25 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "11/22/2022 02:02:25 - INFO - datasets.builder - Generating train split\n",
      "11/22/2022 02:02:25 - INFO - datasets.builder - Generating validation split\n",
      "11/22/2022 02:02:25 - INFO - datasets.builder - Generating test split\n",
      "11/22/2022 02:02:25 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 956.44it/s]\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:25,654 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:25,656 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:448] 2022-11-22 02:02:25,656 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:25,656 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:25,656 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:25,657 >> loading file sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:25,657 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:25,657 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:25,657 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:25,657 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:25,657 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:25,658 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:25,680 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:25,681 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2155] 2022-11-22 02:02:25,737 >> loading weights file models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2600] 2022-11-22 02:02:25,850 >> Some weights of the model checkpoint at models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2612] 2022-11-22 02:02:25,850 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0%|                       | 0/2 [00:00<?, ?ba/s]11/22/2022 02:02:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f2eb5a691cfe1055.arrow\n",
      "Running tokenizer on dataset:  50%|███████▌       | 1/2 [00:00<00:00,  6.42ba/s]\n",
      "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]11/22/2022 02:02:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-1894c993a9bc2db4.arrow\n",
      "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:   0%|                       | 0/2 [00:00<?, ?ba/s]11/22/2022 02:02:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3b76b63892a161a2.arrow\n",
      "Running tokenizer on dataset:  50%|███████▌       | 1/2 [00:00<00:00,  9.10ba/s]\n",
      "11/22/2022 02:02:26 - INFO - __main__ - Sample 1729 of the training set: {'sentence1': \"mais qu'est ce qui lui prend a tom Clancy ??? l'histoire ne décolle pas , c'est mou ,parfois invraisemblable !!!    bref relisez les premiers romans mais pas celui ci !!\", 'label': 0, 'input_ids': [0, 585, 813, 25, 449, 366, 476, 1240, 10039, 10, 2002, 28138, 1476, 11138, 95, 25, 14925, 107, 17844, 890, 402, 6, 4, 432, 25, 449, 13460, 6, 4, 1515, 1979, 159, 23, 310, 5092, 187, 3292, 1581, 3473, 29911, 9213, 7, 907, 191, 22841, 3016, 7, 585, 402, 8029, 791, 3088, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/22/2022 02:02:26 - INFO - __main__ - Sample 788 of the training set: {'sentence1': \"pour débutant c'est super...  Ce n'est pas un livre où il n'y a que des recettes, il reprend des techniques de bases qui se revèlent indispensables pour progresser dans la gastronomie et en cuisine.\", 'label': 1, 'input_ids': [0, 482, 9683, 895, 432, 25, 449, 1092, 27, 1227, 536, 25, 449, 402, 51, 7451, 4693, 200, 536, 25, 53, 10, 41, 210, 18245, 7, 4, 200, 405, 12095, 210, 14276, 8, 13887, 476, 40, 11252, 1274, 7596, 18853, 7, 482, 12265, 56, 637, 21, 15496, 13, 81, 22, 15032, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/22/2022 02:02:26 - INFO - __main__ - Sample 1552 of the training set: {'sentence1': 'C\\'est le deuxième Manchette que je lis (après \"Le petit bleu de la côte Ouest\"), et ça me passionne toujours aussi peu.  Il ne se passe pas grand-chose pendant les deux premiers tiers du bouquin. Vraiment pas mon style de polar.', 'label': 0, 'input_ids': [0, 294, 25, 449, 94, 20754, 1079, 26132, 41, 55, 363, 7, 15, 17979, 44, 2631, 4289, 24285, 8, 21, 2440, 67, 6, 31236, 17834, 81, 1837, 158, 10876, 85, 4618, 2613, 2998, 5, 688, 107, 40, 4512, 402, 4226, 9, 13477, 13, 8174, 191, 3548, 22841, 28742, 113, 7270, 9110, 5, 16184, 14, 550, 402, 1584, 7216, 8, 155, 300, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:727] 2022-11-22 02:02:28,291 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1624] 2022-11-22 02:02:28,294 >> ***** Running training *****\n",
      "[INFO|trainer.py:1625] 2022-11-22 02:02:28,294 >>   Num examples = 1800\n",
      "[INFO|trainer.py:1626] 2022-11-22 02:02:28,294 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1627] 2022-11-22 02:02:28,294 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1628] 2022-11-22 02:02:28,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1629] 2022-11-22 02:02:28,294 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1630] 2022-11-22 02:02:28,294 >>   Total optimization steps = 1125\n",
      "[INFO|trainer.py:1631] 2022-11-22 02:02:28,295 >>   Number of trainable parameters = 23093250\n",
      " 20%|███████▉                                | 224/1125 [00:05<00:18, 47.50it/s][INFO|trainer.py:727] 2022-11-22 02:02:33,703 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:33,705 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:33,705 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:33,705 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5874530673027039, 'eval_accuracy': 0.6700000166893005, 'eval_runtime': 0.1132, 'eval_samples_per_second': 1766.674, 'eval_steps_per_second': 220.834, 'epoch': 1.0}\n",
      " 20%|████████                                | 225/1125 [00:05<00:18, 47.50it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 239.38it/s]\u001b[A\n",
      " 40%|███████████████▊                        | 446/1125 [00:09<00:13, 48.85it/s]\u001b[A[INFO|trainer.py:727] 2022-11-22 02:02:38,206 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:38,207 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:38,207 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:38,207 >>   Batch size = 8\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5372611880302429, 'eval_accuracy': 0.7200000286102295, 'eval_runtime': 0.1015, 'eval_samples_per_second': 1970.396, 'eval_steps_per_second': 246.3, 'epoch': 2.0}\n",
      " 40%|████████████████                        | 450/1125 [00:10<00:13, 48.85it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 258.60it/s]\u001b[A\n",
      "{'loss': 0.5845, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.22}         \u001b[A\n",
      " 60%|███████████████████████▉                | 674/1125 [00:14<00:08, 50.51it/s][INFO|trainer.py:727] 2022-11-22 02:02:42,789 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:42,790 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:42,790 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:42,790 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.45638611912727356, 'eval_accuracy': 0.7699999809265137, 'eval_runtime': 0.1173, 'eval_samples_per_second': 1705.366, 'eval_steps_per_second': 213.171, 'epoch': 3.0}\n",
      " 60%|████████████████████████                | 675/1125 [00:14<00:08, 50.51it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 223.51it/s]\u001b[A\n",
      " 80%|███████████████████████████████▊        | 895/1125 [00:18<00:04, 53.36it/s]\u001b[A[INFO|trainer.py:727] 2022-11-22 02:02:47,240 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:47,241 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:47,241 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:47,241 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.4668016731739044, 'eval_accuracy': 0.7799999713897705, 'eval_runtime': 0.112, 'eval_samples_per_second': 1786.369, 'eval_steps_per_second': 223.296, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████        | 900/1125 [00:19<00:04, 53.36it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 232.41it/s]\u001b[A\n",
      "{'loss': 0.4382, 'learning_rate': 1.2000000000000004e-06, 'epoch': 4.44}        \u001b[A\n",
      "100%|██████████████████████████████████████▉| 1123/1125 [00:23<00:00, 50.88it/s][INFO|trainer.py:727] 2022-11-22 02:02:51,788 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:51,789 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:51,789 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:51,789 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.43559274077415466, 'eval_accuracy': 0.7950000166893005, 'eval_runtime': 0.1054, 'eval_samples_per_second': 1898.24, 'eval_steps_per_second': 237.28, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1125/1125 [00:23<00:00, 50.88it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 249.91it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1875] 2022-11-22 02:02:51,895 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 23.601, 'train_samples_per_second': 381.34, 'train_steps_per_second': 47.667, 'train_loss': 0.5009121975368923, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1125/1125 [00:23<00:00, 47.68it/s]\n",
      "[INFO|trainer.py:2694] 2022-11-22 02:02:51,897 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0\n",
      "[INFO|configuration_utils.py:447] 2022-11-22 02:02:51,897 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-22 02:02:51,975 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-22 02:02:51,975 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-22 02:02:51,975 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.5009\n",
      "  train_runtime            = 0:00:23.60\n",
      "  train_samples            =       1800\n",
      "  train_samples_per_second =     381.34\n",
      "  train_steps_per_second   =     47.667\n",
      "11/22/2022 02:02:52 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:727] 2022-11-22 02:02:52,001 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:52,001 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:52,001 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:52,001 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 270.85it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =      0.795\n",
      "  eval_loss               =     0.4356\n",
      "  eval_runtime            = 0:00:00.09\n",
      "  eval_samples            =        200\n",
      "  eval_samples_per_second =   2087.783\n",
      "  eval_steps_per_second   =    260.973\n",
      "11/22/2022 02:02:52 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:727] 2022-11-22 02:02:52,099 >> The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:02:52,100 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:02:52,100 >>   Num examples = 2000\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:02:52,100 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████| 250/250 [00:01<00:00, 221.48it/s]\n",
      "11/22/2022 02:02:53 - INFO - __main__ - ***** Predict results None *****\n",
      "[INFO|modelcard.py:449] 2022-11-22 02:02:53,646 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7950000166893005}]}\n",
      "11/22/2022 02:02:56 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/22/2022 02:02:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.98,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/runs/Nov22_02-02-55_AROZYM,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=polynomial,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=1,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/22/2022 02:02:56 - INFO - __main__ - load a local file for train: data/cls.books-json/train.json\n",
      "11/22/2022 02:02:56 - INFO - __main__ - load a local file for validation: data/cls.books-json/valid.json\n",
      "11/22/2022 02:02:56 - INFO - __main__ - load a local file for test: data/cls.books-json/test.json\n",
      "11/22/2022 02:02:56 - WARNING - datasets.builder - Using custom data configuration default-403939cb3edeb7bb\n",
      "11/22/2022 02:02:56 - INFO - datasets.info - Loading Dataset Infos from /home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "11/22/2022 02:02:56 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/22/2022 02:02:56 - INFO - datasets.info - Loading Dataset info from /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
      "11/22/2022 02:02:56 - WARNING - datasets.builder - Found cached dataset json (/home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "11/22/2022 02:02:56 - INFO - datasets.info - Loading Dataset info from /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 995.64it/s]\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:56,634 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:56,637 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:448] 2022-11-22 02:02:56,637 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:56,637 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:56,637 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:56,638 >> loading file sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:56,638 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:56,638 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:56,638 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:02:56,638 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:56,638 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:56,638 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:02:56,664 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:02:56,664 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2155] 2022-11-22 02:02:56,722 >> loading weights file models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2600] 2022-11-22 02:02:56,850 >> Some weights of the model checkpoint at models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2612] 2022-11-22 02:02:56,850 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/22/2022 02:02:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f2eb5a691cfe1055.arrow\n",
      "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]11/22/2022 02:02:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-08ba46085ad91e9f.arrow\n",
      "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]\n",
      "11/22/2022 02:02:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3b76b63892a161a2.arrow\n",
      "11/22/2022 02:02:56 - INFO - __main__ - Sample 275 of the training set: {'sentence1': 'On croit bien évidemment tout savoir sur l\\'horreur indicible des camps de concentration. Et bien non, \"si c\\'est un homme\" laisse littéralement sans voix ! Tout commentaire paraît déplacé et pour tout dire insignifiant. Lisez, vous comprendrez.  Mon seul commentaire est donc qu\\'il faut impérativement recommander la lecture de cet ouvrage à tous et particulièrement aux grands adolescents.', 'label': 1, 'input_ids': [0, 1360, 5181, 204, 1202, 27809, 1530, 6958, 511, 95, 25, 2893, 106, 415, 26070, 1581, 210, 4689, 7, 8, 30525, 5, 1183, 1202, 324, 4, 44, 167, 432, 25, 449, 51, 13971, 58, 17087, 2845, 14397, 7041, 2814, 22356, 573, 9757, 27288, 119, 8412, 16884, 138, 4747, 81, 482, 1530, 2664, 30128, 25306, 5, 21780, 164, 4, 773, 16504, 164, 5, 3424, 10724, 27288, 390, 3882, 813, 25, 346, 4219, 7578, 2446, 12754, 550, 29040, 42, 21, 15429, 8, 6572, 6, 25818, 239, 2862, 81, 6, 20707, 1723, 17041, 29943, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/22/2022 02:02:56 - INFO - __main__ - Sample 1165 of the training set: {'sentence1': \"L'intrigue est faible, mais l'epopee dans la jungle est divertissante, un peu longue peut-etre; la fin est sans surprise.\", 'label': 0, 'input_ids': [0, 313, 25, 31850, 390, 22315, 4, 585, 95, 25, 13, 10337, 3264, 637, 21, 16391, 130, 390, 22248, 16220, 13, 4, 51, 2998, 18512, 2293, 9, 123, 106, 73, 21, 1408, 390, 2814, 14727, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/22/2022 02:02:56 - INFO - __main__ - Sample 1735 of the training set: {'sentence1': \"Rien ne se tient et rien ne va dans ce Jésus parlait Araméen. Pourtant, l'idée à l'origine du livre était bonne : Jésus, lorsqu'il enseignait, parlait un dialecte hébraïque, l'Araméen palestinien. Or, son enseignement nous est parvenu dans les traductions grecques des textes du Nouveau Testament, les originaux hébreux ayant été égarés. De fait, l'annonce par l'auteur de la découverte d'un Evangile écrit en Araméen autorisait une relecture des textes grecs - car on sait qu'une traduction transforme le sens d'un texte et l'altère. Revenir donc aux sources, en comparant Grec et Araméen, aurait pu permettre de relire les paroles de Jésus en collant encore plus à ce qu'il avait réellement voulu dire.  Mais, hélas, Eric Edelmann, l'auteur du livre, n'a en fait rien fait de l'idée de départ de son propre ouvrage ! Au lieu de retraduire ou repenser les Evangiles pour en donner un sens plus précis, il prend une option philosophiquement acceptable, mais indéfendable d'un point de vue chrétien, le gnosticisme - position qu'il récuse, persuadé de la différence entre la gnose (connaissance) et gnosticisme (religion basée sur la connaissance). Selon Edelmann, en effet, Jésus a donné un enseignement permettant à tout disciple le suivant de réaliser sa transformation intérieure, de prendre la voie de l'élévation spirituelle. Or, l'erreur majeure de l'auteur aura été de considérer que cette transformation est indépendante du caeur du message du Christ. Ainsi, Edelmann fait fi du plus grand enseignement de la Thora revendiqué par Jésus : aimer le seul et unique Dieu, et aimer son prochain comme sois même. Car l'amour, cet amour totale et entier à la base du Christianisme reste la condition sine qua non pour que tout un chacun, nous nous retrouvions réellement transformés. Sans compter qu'au-delà même de cet amour humain, il y a l'Amour de Dieu : Jésus, en ressuscitant, trompe la mort et donne à chacun la garantie de l'immortalité de l'âme - d'où le sacrifice du Christ, sacrifice d'Amour pur. Il refuse l'idée de résurrection du Christ, préférant réduire son triomphe sur la mort à une maîtrise de son corps comme des grands maîtres tibétains ou yogis. Edelmann occulte donc volontairement les apparitions du Christ aux douze et à Maryam et ce sans la moindre justification.  De fait, en occultant la base même du Christianisme, Eric Edelmann se trompe complètement sur le message de Jésus. Il relit les paraboles, et tente d'y retrouver le sens premier du texte. Mais, il a beau se référer à l'Araméen, en réalité, le nouveau sens dégagé n'est pas toujours éloigné du précédent contrairement à ce qu'il prétend - en l'occurrence, soulignons le, il apporte néanmoins de nombreux points de vues intéressants sur les termes utilisés dans le Nouveau Testament, permettant au néophyte d'appréhender un texte plus difficile qu'on ne le croit ; on apprécie aussi le sort qu'il fait à la traduction de la Bible par la Bible de Jérusalem : les faiblesses sont patentes et on apprécie de se voir mis en garde. Mais Edelmann, hélas, pousse loin les choses en prétendant que pour Jésus, il n'y a ni Bien ni Mal, mais juste une bonne façon d'avancer sur le chemin intérieur. Quelle fatuité ! Comment alors cataloguer le chemin qui ne conduit pas au bien être comparé à celui conduisant à la pureté : ne sont-ce pas le Mal et le Bien ? Cette prétention ridicule à nier l'idée de pêché, à nier l'idée de faute, revient à faire de Jésus un simple gourou. Il le met d'ailleurs sur le même plan que les maître indiens : après tout, pourquoi pas proposer un dialogue interreligieux ; pourquoi pas donner une lecture spirituelle de Jésus. Mais en tout cas, il ne jamais espérer ou croire pouvoir devenir meilleur, plus maître de soi, plus noble et sage en oubliant que l'Amour se révèle la seule clé permettant notre transformation.  Cela ne rate pas : la relecture des paraboles évangéliques d'Edelmann ne tiennent pas la route une seule seconde. Disons le : on ne retiens rien de ses verbiages ! En effet, l'auteur ne cesse de répéter tout au long du livre que les propos de Jésus sont fort clairs, et ses instructions précises. Mais en le lisant, on ne voit absolument pas comment faire pour atteindre cet état d'éveil ! Oh, certes, on a compris qu'il fallait apprendre à considérer les choses sous un angle juste, faire preuve d'humilité, apprendre à faire confiance à notre voix intérieur, accepter de nous reconnaître comme faibles du point de vue psychique et travailler sur nos résistances, mais avait-on besoin de ce livre pour cela ? La réponse est non : car au-delà de toutes les erreurs de l'auteur, il oublie qu'un enseignement de type gourou passe par l'oral : que Jésus a eu beau écrire tout ce qu'il pouvait écrire, ses paroles resteront à jamais lettre morte pour celui refusant d'accepter la réalité de l'Amour de Dieu.  A l'origine des premières Eglises, les gnostiques apparurent : ils récupérèrent la figure du Christ pour en faire le héraut de leur enseignement. Refusant de reconnaître que Dieu était le père du Monde, il croyait qu'il y avait deux entités, dont l'une cruelle avait créé la matière et qu'il fallait rejeter le corps pour atteindre la libération par la mort, voir son esprit voler dans les étoiles, et retrouver la demeure des dieux. Ils furent considérés comme hérétiques puisque leurs traditions et leurs philosophies étaient en contradiction avec l'essence même du Christianisme... Aujourd'hui encore, ce mouvement n'est pas mort : ce livre en atteste. Tant qu'on lira les paroles du Christ en étant persuadé qu'on peut se passer d'aimer, qu'on peut se contenter d'atteindre l'éveil en acquérant des connaissances, on restera un gnostique, un être incomplet, incapable de se libérer des réelles chaînes de l'Etre. Et l'auteur aura beau condamner le gnosticisme et célébrer la gnose, cela ne change rien : il pêche, soit selon le sens étymologique du mot, rate sa cible.\", 'label': 0, 'input_ids': [0, 27301, 107, 40, 27616, 81, 5794, 107, 288, 637, 366, 31888, 8658, 204, 4373, 4387, 33, 5, 30401, 4, 95, 25, 17495, 239, 95, 25, 17168, 113, 7451, 5518, 6649, 147, 31888, 4, 22011, 25, 346, 22, 7, 11403, 4204, 4, 8658, 204, 51, 31256, 13, 10792, 1671, 4069, 724, 4, 95, 25, 268, 1376, 398, 33, 17091, 3116, 5, 1880, 4, 617, 6, 21486, 1326, 390, 23831, 459, 637, 191, 17334, 2627, 24474, 3312, 210, 12317, 7, 113, 31619, 16412, 4, 191, 15862, 2943, 10792, 2194, 2943, 15873, 2649, 2353, 142, 1399, 5, 248, 1820, 4, 95, 25, 22543, 335, 95, 25, 15608, 8, 21, 22362, 103, 25, 290, 22439, 11504, 15713, 22, 4373, 4387, 33, 7427, 7, 4204, 616, 405, 3937, 3139, 210, 12317, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
      "[INFO|trainer.py:727] 2022-11-22 02:02:59,050 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1624] 2022-11-22 02:02:59,052 >> ***** Running training *****\n",
      "[INFO|trainer.py:1625] 2022-11-22 02:02:59,052 >>   Num examples = 1800\n",
      "[INFO|trainer.py:1626] 2022-11-22 02:02:59,052 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1627] 2022-11-22 02:02:59,052 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1628] 2022-11-22 02:02:59,052 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1629] 2022-11-22 02:02:59,052 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1630] 2022-11-22 02:02:59,052 >>   Total optimization steps = 1125\n",
      "[INFO|trainer.py:1631] 2022-11-22 02:02:59,053 >>   Number of trainable parameters = 23093250\n",
      " 20%|████████                                | 225/1125 [00:05<00:19, 46.98it/s][INFO|trainer.py:727] 2022-11-22 02:03:04,673 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:04,674 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:04,674 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:04,674 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.59543776512146, 'eval_accuracy': 0.6949999928474426, 'eval_runtime': 0.1169, 'eval_samples_per_second': 1710.292, 'eval_steps_per_second': 213.787, 'epoch': 1.0}\n",
      " 20%|████████                                | 225/1125 [00:05<00:19, 46.98it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 221.95it/s]\u001b[A\n",
      " 40%|███████████████▉                        | 449/1125 [00:10<00:13, 48.66it/s][INFO|trainer.py:727] 2022-11-22 02:03:09,668 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:09,669 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:09,669 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:09,669 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5077767968177795, 'eval_accuracy': 0.75, 'eval_runtime': 0.1286, 'eval_samples_per_second': 1555.765, 'eval_steps_per_second': 194.471, 'epoch': 2.0}\n",
      " 40%|████████████████                        | 450/1125 [00:10<00:13, 48.66it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 195.11it/s]\u001b[A\n",
      "{'loss': 0.5809, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.22}         \u001b[A\n",
      " 60%|███████████████████████▊                | 671/1125 [00:15<00:08, 51.10it/s][INFO|trainer.py:727] 2022-11-22 02:03:14,493 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:14,494 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:14,494 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:14,494 >>   Batch size = 8\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5148534178733826, 'eval_accuracy': 0.7549999952316284, 'eval_runtime': 0.1027, 'eval_samples_per_second': 1946.859, 'eval_steps_per_second': 243.357, 'epoch': 3.0}\n",
      " 60%|████████████████████████                | 675/1125 [00:15<00:08, 51.10it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 253.65it/s]\u001b[A\n",
      " 80%|████████████████████████████████        | 900/1125 [00:20<00:05, 42.24it/s][INFO|trainer.py:727] 2022-11-22 02:03:19,316 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:19,317 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:19,317 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:19,317 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.48338577151298523, 'eval_accuracy': 0.7699999809265137, 'eval_runtime': 0.1411, 'eval_samples_per_second': 1417.165, 'eval_steps_per_second': 177.146, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████        | 900/1125 [00:20<00:05, 42.24it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 170.10it/s]\u001b[A\n",
      "{'loss': 0.4503, 'learning_rate': 1.2000000000000004e-06, 'epoch': 4.44}        \u001b[A\n",
      "100%|██████████████████████████████████████▉| 1122/1125 [00:25<00:00, 43.34it/s][INFO|trainer.py:727] 2022-11-22 02:03:24,455 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:24,456 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:24,456 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:24,456 >>   Batch size = 8\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.44385087490081787, 'eval_accuracy': 0.8050000071525574, 'eval_runtime': 0.1012, 'eval_samples_per_second': 1977.079, 'eval_steps_per_second': 247.135, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1125/1125 [00:25<00:00, 43.34it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 258.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1875] 2022-11-22 02:03:24,558 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 25.5056, 'train_samples_per_second': 352.864, 'train_steps_per_second': 44.108, 'train_loss': 0.5007754991319444, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1125/1125 [00:25<00:00, 44.12it/s]\n",
      "[INFO|trainer.py:2694] 2022-11-22 02:03:24,560 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1\n",
      "[INFO|configuration_utils.py:447] 2022-11-22 02:03:24,560 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-22 02:03:24,641 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-22 02:03:24,642 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-22 02:03:24,642 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.5008\n",
      "  train_runtime            = 0:00:25.50\n",
      "  train_samples            =       1800\n",
      "  train_samples_per_second =    352.864\n",
      "  train_steps_per_second   =     44.108\n",
      "11/22/2022 02:03:24 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:727] 2022-11-22 02:03:24,666 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:24,667 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:24,667 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:24,667 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 229.63it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =      0.805\n",
      "  eval_loss               =     0.4439\n",
      "  eval_runtime            = 0:00:00.11\n",
      "  eval_samples            =        200\n",
      "  eval_samples_per_second =    1777.79\n",
      "  eval_steps_per_second   =    222.224\n",
      "11/22/2022 02:03:24 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:727] 2022-11-22 02:03:24,781 >> The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:24,782 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:24,782 >>   Num examples = 2000\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:24,782 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████| 250/250 [00:01<00:00, 223.07it/s]\n",
      "11/22/2022 02:03:25 - INFO - __main__ - ***** Predict results None *****\n",
      "[INFO|modelcard.py:449] 2022-11-22 02:03:26,344 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8050000071525574}]}\n",
      "11/22/2022 02:03:28 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/22/2022 02:03:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.98,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/runs/Nov22_02-03-28_AROZYM,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=polynomial,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=2,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/22/2022 02:03:28 - INFO - __main__ - load a local file for train: data/cls.books-json/train.json\n",
      "11/22/2022 02:03:28 - INFO - __main__ - load a local file for validation: data/cls.books-json/valid.json\n",
      "11/22/2022 02:03:28 - INFO - __main__ - load a local file for test: data/cls.books-json/test.json\n",
      "11/22/2022 02:03:29 - WARNING - datasets.builder - Using custom data configuration default-403939cb3edeb7bb\n",
      "11/22/2022 02:03:29 - INFO - datasets.info - Loading Dataset Infos from /home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "11/22/2022 02:03:29 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/22/2022 02:03:29 - INFO - datasets.info - Loading Dataset info from /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
      "11/22/2022 02:03:29 - WARNING - datasets.builder - Found cached dataset json (/home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "11/22/2022 02:03:29 - INFO - datasets.info - Loading Dataset info from /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1645.04it/s]\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:03:29,243 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:03:29,245 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:448] 2022-11-22 02:03:29,245 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:03:29,245 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:03:29,246 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:03:29,246 >> loading file sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:03:29,246 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:03:29,246 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:03:29,246 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 02:03:29,246 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:03:29,246 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:03:29,247 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:652] 2022-11-22 02:03:29,270 >> loading configuration file models/RoBERTa_small_fr_HuggingFace/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 02:03:29,271 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models/RoBERTa_small_fr_HuggingFace\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2155] 2022-11-22 02:03:29,345 >> loading weights file models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2600] 2022-11-22 02:03:29,461 >> Some weights of the model checkpoint at models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2612] 2022-11-22 02:03:29,461 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/22/2022 02:03:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f2eb5a691cfe1055.arrow\n",
      "11/22/2022 02:03:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-08ba46085ad91e9f.arrow\n",
      "Running tokenizer on dataset:   0%|                       | 0/2 [00:00<?, ?ba/s]11/22/2022 02:03:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/baptiste/.cache/huggingface/datasets/json/default-403939cb3edeb7bb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a558c97d7cd78cd5.arrow\n",
      "Running tokenizer on dataset:  50%|███████▌       | 1/2 [00:00<00:00,  5.48ba/s]\n",
      "11/22/2022 02:03:29 - INFO - __main__ - Sample 1767 of the training set: {'sentence1': \"Les trois premiers tomes m'ont totalement transporté! j'ai été époustoufflé par les rebondissements, dans le tome 4 je retrouve le plaisir de lire le style d'écriture de l'autrice mais alors qu'est ce que je suis déçu par le contenu. J'ai mis un jour pour lire chaque tome et le quatrième je l'ai lu en une semaine tellement chaque chapitre me désespéraient c'est pour dire! Certes beaucoup de rebondissements encore mais tellement décevant, cela devient rapidement inintéressant en fait.\", 'label': 0, 'input_ids': [0, 1161, 9488, 22841, 7509, 7, 321, 25, 3588, 25440, 2976, 398, 38, 1117, 25, 437, 2649, 356, 615, 16167, 7455, 2034, 335, 191, 405, 14232, 13979, 7, 4, 637, 94, 7509, 193, 55, 20917, 94, 10640, 8, 13123, 94, 7216, 103, 25, 29890, 8, 95, 25, 703, 6819, 585, 5700, 813, 25, 449, 366, 41, 55, 2608, 1904, 6492, 335, 94, 15488, 5, 644, 25, 437, 898, 51, 4841, 482, 13123, 6866, 7509, 81, 94, 1649, 2143, 8038, 55, 95, 25, 437, 1938, 22, 616, 10958, 20945, 6866, 31094, 158, 14479, 89, 4803, 18636, 432, 25, 449, 482, 2664, 38, 3854, 1109, 5239, 8, 405, 14232, 13979, 7, 3888, 585, 20945, 8920, 13, 9661, 4, 4481, 26541, 16622, 23, 2259, 3638, 16220, 22, 1820, 5, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}.\n",
      "11/22/2022 02:03:29 - INFO - __main__ - Sample 1738 of the training set: {'sentence1': 'Livre intéressant, avec pas mal d\\'humour, mais il y a un gros \"mais\".   Après avoir lu le principe de lucifer volume 1 et 2 de bloom, on peut penser que la théorie centrale de ce livre est discutable, voire vraiment incomplète. Le groupe influe énormément sur nos comportements, il n\\'y a pas que nos gènes!   Donc lisez ce livre, puis lisez le principe de lucifer pour une vision différente.', 'label': 0, 'input_ids': [0, 29601, 19111, 4, 1098, 402, 732, 103, 25, 30494, 4, 585, 200, 112, 10, 51, 11492, 44, 5915, 592, 12131, 4858, 1938, 94, 15866, 8, 18826, 1686, 7570, 105, 81, 114, 8, 9444, 287, 4, 97, 2293, 21023, 41, 21, 31406, 11809, 8, 366, 7451, 390, 22163, 1581, 4, 5226, 13, 6718, 19186, 10703, 67, 5, 526, 11224, 8749, 13, 31872, 511, 727, 26569, 7, 4, 200, 536, 25, 53, 10, 402, 41, 727, 6, 25426, 7, 38, 16214, 23947, 164, 366, 7451, 4, 7784, 23947, 164, 94, 15866, 8, 18826, 1686, 482, 616, 11240, 26629, 13, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/22/2022 02:03:29 - INFO - __main__ - Sample 115 of the training set: {'sentence1': 'Livre absolument superbe avec des maisons ravissantes, des personnages superjolis. A avoir dans sa bibliothèque !', 'label': 1, 'input_ids': [0, 29601, 28596, 25144, 1098, 210, 7781, 7, 10903, 16220, 89, 4, 210, 19371, 7, 1092, 440, 2175, 5, 62, 4858, 637, 57, 31092, 573, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:727] 2022-11-22 02:03:31,688 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/baptiste/.cache/pypoetry/virtualenvs/altegrad-lab3-FbjClyzN-py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1624] 2022-11-22 02:03:31,690 >> ***** Running training *****\n",
      "[INFO|trainer.py:1625] 2022-11-22 02:03:31,690 >>   Num examples = 1800\n",
      "[INFO|trainer.py:1626] 2022-11-22 02:03:31,690 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1627] 2022-11-22 02:03:31,690 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1628] 2022-11-22 02:03:31,690 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1629] 2022-11-22 02:03:31,690 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1630] 2022-11-22 02:03:31,690 >>   Total optimization steps = 1125\n",
      "[INFO|trainer.py:1631] 2022-11-22 02:03:31,691 >>   Number of trainable parameters = 23093250\n",
      " 20%|███████▉                                | 224/1125 [00:05<00:19, 46.03it/s][INFO|trainer.py:727] 2022-11-22 02:03:37,101 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:37,102 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:37,102 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:37,102 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5690901875495911, 'eval_accuracy': 0.699999988079071, 'eval_runtime': 0.1078, 'eval_samples_per_second': 1854.91, 'eval_steps_per_second': 231.864, 'epoch': 1.0}\n",
      " 20%|████████                                | 225/1125 [00:05<00:19, 46.03it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 243.61it/s]\u001b[A\n",
      " 40%|███████████████▉                        | 449/1125 [00:10<00:14, 46.19it/s][INFO|trainer.py:727] 2022-11-22 02:03:41,953 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:41,954 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:41,954 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:41,954 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.521502673625946, 'eval_accuracy': 0.7300000190734863, 'eval_runtime': 0.1365, 'eval_samples_per_second': 1464.824, 'eval_steps_per_second': 183.103, 'epoch': 2.0}\n",
      " 40%|████████████████                        | 450/1125 [00:10<00:14, 46.19it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 205.21it/s]\u001b[A\n",
      "{'loss': 0.5692, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.22}         \u001b[A\n",
      " 60%|███████████████████████▉                | 673/1125 [00:15<00:09, 48.63it/s][INFO|trainer.py:727] 2022-11-22 02:03:46,919 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:46,920 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:46,920 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:46,920 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.46521812677383423, 'eval_accuracy': 0.7699999809265137, 'eval_runtime': 0.1631, 'eval_samples_per_second': 1226.396, 'eval_steps_per_second': 153.299, 'epoch': 3.0}\n",
      " 60%|████████████████████████                | 675/1125 [00:15<00:09, 48.63it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 179.09it/s]\u001b[A\n",
      " 80%|████████████████████████████████        | 900/1125 [00:19<00:04, 49.52it/s][INFO|trainer.py:727] 2022-11-22 02:03:51,587 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:51,588 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:51,588 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:51,588 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.48073041439056396, 'eval_accuracy': 0.7649999856948853, 'eval_runtime': 0.1081, 'eval_samples_per_second': 1849.581, 'eval_steps_per_second': 231.198, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████        | 900/1125 [00:19<00:04, 49.52it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 244.23it/s]\u001b[A\n",
      "{'loss': 0.4472, 'learning_rate': 1.2000000000000004e-06, 'epoch': 4.44}        \u001b[A\n",
      "100%|██████████████████████████████████████▊| 1121/1125 [00:24<00:00, 49.87it/s][INFO|trainer.py:727] 2022-11-22 02:03:56,281 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:56,282 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:56,282 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:56,282 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.44012755155563354, 'eval_accuracy': 0.7850000262260437, 'eval_runtime': 0.1065, 'eval_samples_per_second': 1878.087, 'eval_steps_per_second': 234.761, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1125/1125 [00:24<00:00, 49.87it/s]\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 248.64it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1875] 2022-11-22 02:03:56,389 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 24.6987, 'train_samples_per_second': 364.391, 'train_steps_per_second': 45.549, 'train_loss': 0.4957624308268229, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1125/1125 [00:24<00:00, 45.56it/s]\n",
      "[INFO|trainer.py:2694] 2022-11-22 02:03:56,390 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2\n",
      "[INFO|configuration_utils.py:447] 2022-11-22 02:03:56,391 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-22 02:03:56,473 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-22 02:03:56,474 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-22 02:03:56,474 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.4958\n",
      "  train_runtime            = 0:00:24.69\n",
      "  train_samples            =       1800\n",
      "  train_samples_per_second =    364.391\n",
      "  train_steps_per_second   =     45.549\n",
      "11/22/2022 02:03:56 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:727] 2022-11-22 02:03:56,499 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:56,500 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:56,500 >>   Num examples = 200\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:56,500 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 25/25 [00:00<00:00, 253.50it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =      0.785\n",
      "  eval_loss               =     0.4401\n",
      "  eval_runtime            = 0:00:00.10\n",
      "  eval_samples            =        200\n",
      "  eval_samples_per_second =    1954.94\n",
      "  eval_steps_per_second   =    244.367\n",
      "11/22/2022 02:03:56 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:727] 2022-11-22 02:03:56,604 >> The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2945] 2022-11-22 02:03:56,605 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2947] 2022-11-22 02:03:56,605 >>   Num examples = 2000\n",
      "[INFO|trainer.py:2950] 2022-11-22 02:03:56,605 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████| 250/250 [00:01<00:00, 224.48it/s]\n",
      "11/22/2022 02:03:57 - INFO - __main__ - ***** Predict results None *****\n",
      "[INFO|modelcard.py:449] 2022-11-22 02:03:58,132 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7850000262260437}]}\n"
     ]
    }
   ],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
    "    --model_name_or_path $MODEL_PATH \\\n",
    "    --train_file $TRAIN_FILE \\\n",
    "    --validation_file $VALIDATION_FILE \\\n",
    "    --test_file $TEST_FILE \\\n",
    "    --output_dir $SAVE_DIR \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --per_device_train_batch_size $MAX_SENTENCES \\\n",
    "    --per_device_eval_batch_size $MAX_SENTENCES \\\n",
    "    --learning_rate $LR \\\n",
    "    --lr_scheduler_type polynomial \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --optim adamw_hf \\\n",
    "    --adam_beta1 0.9 \\\n",
    "    --adam_beta2 0.98 \\\n",
    "    --adam_epsilon 1e-08 \\\n",
    "    --num_train_epochs $MAX_EPOCH \\\n",
    "    --seed $SEED \\\n",
    "    --save_strategy no \\\n",
    "  )#fill me "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2UMHjatpFvm"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir checkpoints"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.7 ('altegrad-lab3-FbjClyzN-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bdbeb7ff11ae5d2b59bb4c0bd1de7741ee72e03cff1ba6e7a3d29fd1e7f326f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
